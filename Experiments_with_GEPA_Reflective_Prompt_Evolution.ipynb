{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBVbmE-a0O9e"
      },
      "source": [
        "## GEPA - Genetic Pareto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "leBJJOlf0MhZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(62075) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install -q google-generativeai datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "tm7XVQcs6tnD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(62077) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install litellm pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAJKG_-x5oEf"
      },
      "source": [
        "#### Define API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "3enAptL75qUA"
      },
      "outputs": [],
      "source": [
        "# --- API Keys (loaded from Colab Secrets) ---\n",
        "try:\n",
        "    # NOTE: Only a Google/Gemini API key is needed now.\n",
        "    GEMINI_API_KEY = \"AIzaSyDeiGggyC3hrHTOhnL35j6inxjm8ocdcxU\"  #userdata.get('GEMINI_API_KEY')\n",
        "except Exception as e:\n",
        "    print(\"Could not load API key from Colab Secrets. Please ensure 'GEMINI_API_KEY' is set.\")\n",
        "    # Terminate execution if key is not found\n",
        "    GEMINI_API_KEY = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "ZraJwfO6DSgV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tvl3UWJduD_"
      },
      "source": [
        "#### Use HF inference endpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "6GGOldJybPHK"
      },
      "outputs": [],
      "source": [
        "# # --- 1. Installation and Imports ---\n",
        "# # Install necessary libraries in the Colab environment\n",
        "# !pip install -q huggingface_hub google-generativeai\n",
        "\n",
        "# import os\n",
        "# import json\n",
        "# import random\n",
        "# import time\n",
        "# from huggingface_hub import InferenceClient\n",
        "# import google.generativeai as genai\n",
        "# from google.colab import userdata # For securely accessing API keys\n",
        "\n",
        "# # --- Helper Functions ---\n",
        "\n",
        "# def log_message(message, type='info'):\n",
        "#     \"\"\"Helper to format log messages with a timestamp.\"\"\"\n",
        "#     timestamp = time.strftime(\"%H:%M:%S\")\n",
        "#     if type == 'success':\n",
        "#         return f\"[{timestamp}] ✅ SUCCESS: {message}\"\n",
        "#     if type == 'fail':\n",
        "#         return f\"[{timestamp}] ❌ FAIL: {message}\"\n",
        "#     if type == 'best':\n",
        "#         return f\"[{timestamp}] ⭐ BEST: {message}\"\n",
        "#     return f\"[{timestamp}] ℹ️ INFO: {message}\"\n",
        "\n",
        "# # --- Core GEPA Functions (Python Implementation) ---\n",
        "\n",
        "# def run_huggingface_rollout(client, model_id, prompt, input_text):\n",
        "#     \"\"\"\n",
        "#     Calls the Hugging Face Inference API for the target model.\n",
        "#     This function performs a \"rollout\" for a given prompt and input.\n",
        "#     \"\"\"\n",
        "#     # Check if model is a chat model and format accordingly\n",
        "#     if \"gemma\" in model_id.lower() or \"llama\" in model_id.lower() or \"chat\" in model_id.lower():\n",
        "#         messages = [\n",
        "#             {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nText: \\\"{input_text}\\\"\"}\n",
        "#         ]\n",
        "#         try:\n",
        "#             response = client.chat_completion(\n",
        "#                 model=model_id,\n",
        "#                 messages=messages,\n",
        "#                 max_tokens=100,\n",
        "#                 temperature=0.7,\n",
        "#                 top_p=0.95\n",
        "#             )\n",
        "#             return response.choices[0].message.content\n",
        "#         except Exception as e:\n",
        "#             # Fallback to text generation if chat completion fails for any reason\n",
        "#             pass\n",
        "\n",
        "#     # Use standard text generation format\n",
        "#     full_prompt = f\"{prompt}\\n\\nText: \\\"{input_text}\\\"\\n\\nResponse:\"\n",
        "#     try:\n",
        "#         response = client.text_generation(\n",
        "#             model=model_id,\n",
        "#             prompt=full_prompt,\n",
        "#             max_new_tokens=100,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.7,\n",
        "#             top_p=0.95,\n",
        "#             return_full_text=False\n",
        "#         )\n",
        "#         return response\n",
        "#     except Exception as e:\n",
        "#         err_str = str(e).lower()\n",
        "#         if \"authorization\" in err_str or \"401\" in err_str:\n",
        "#             raise Exception(f\"Hugging Face API Error: Authorization failed. Ensure your HF Token is correct and you have accepted the terms for '{model_id}'.\")\n",
        "#         if \"not found\" in err_str or \"404\" in err_str:\n",
        "#             raise Exception(f\"Hugging Face API Error: Model '{model_id}' not found or requires a Pro subscription.\")\n",
        "#         if \"rate limit\" in err_str or \"429\" in err_str:\n",
        "#             raise Exception(f\"Hugging Face API Error: Rate limit exceeded. Please wait and try again.\")\n",
        "#         if \"loading\" in err_str or \"503\" in err_str:\n",
        "#             raise Exception(f\"Hugging Face API Error: Model '{model_id}' is currently loading. Wait a few minutes and try again.\")\n",
        "#         raise Exception(f\"Hugging Face API Error: {str(e)}\")\n",
        "\n",
        "\n",
        "# def evaluation_and_feedback_function(output, task):\n",
        "#     \"\"\"\n",
        "#     The evaluation function (μ_f in the paper). Scores the model's output\n",
        "#     and provides textual feedback.\n",
        "#     IMPORTANT: This is the most critical part to customize for a specific task.\n",
        "#     \"\"\"\n",
        "#     if not output or not isinstance(output, str):\n",
        "#         return {\"score\": 0.0, \"feedback\": \"No valid output generated.\"}\n",
        "\n",
        "#     # --- CUSTOMIZE THIS FUNCTION ---\n",
        "#     # This example checks for keyword presence. For a real task, you might use\n",
        "#     # regex, semantic similarity, code compilation, etc.\n",
        "#     score = 0.0\n",
        "#     feedback = \"\"\n",
        "#     found_keywords = 0\n",
        "#     expected_keywords = task.get(\"expected_keywords\", [])\n",
        "\n",
        "#     if not expected_keywords:\n",
        "#         return {\"score\": 0.0, \"feedback\": \"No evaluation criteria (expected_keywords) found in training data.\"}\n",
        "\n",
        "#     for keyword in expected_keywords:\n",
        "#         if keyword.lower() in output.lower():\n",
        "#             found_keywords += 1\n",
        "#             feedback += f\"SUCCESS: Output correctly contained '{keyword}'.\\n\"\n",
        "#         else:\n",
        "#             feedback += f\"FAILURE: Output was missing required keyword '{keyword}'.\\n\"\n",
        "\n",
        "#     score = found_keywords / len(expected_keywords) if expected_keywords else 0.0\n",
        "#     feedback += f\"Final Score for this task: {score:.2f}\"\n",
        "#     return {\"score\": score, \"feedback\": feedback}\n",
        "#     # --- END CUSTOMIZATION ---\n",
        "\n",
        "\n",
        "# def reflect_and_propose_new_prompt(gemini_model, current_prompt, examples):\n",
        "#     \"\"\"\n",
        "#     Performs the Reflective Prompt Mutation step using a powerful LLM (Gemini).\n",
        "#     \"\"\"\n",
        "#     examples_text = '---'.join(\n",
        "#         f'Task Input: \"{e[\"input\"]}\"\\nGenerated Output: \"{e[\"output\"]}\"\\nFeedback:\\n{e[\"feedback\"]}\\n\\n'\n",
        "#         for e in examples\n",
        "#     )\n",
        "\n",
        "#     reflection_prompt = f\"\"\"You are an expert prompt engineer. Your task is to refine a prompt to improve its performance based on feedback from previous attempts.\n",
        "\n",
        "# Here is the current prompt that needs improvement:\n",
        "# --- CURRENT PROMPT ---\n",
        "# {current_prompt}\n",
        "# --------------------\n",
        "\n",
        "# Here are examples of how the prompt performed on a few tasks, along with feedback on what went wrong or right:\n",
        "# --- EXAMPLES & FEEDBACK ---\n",
        "# {examples_text}\n",
        "# -------------------------\n",
        "\n",
        "# Based on this analysis, your task is to write a new, improved prompt. The new prompt should be a complete set of instructions that directly addresses the failures and incorporates the successful strategies observed in the feedback. Do not just give suggestions; provide the full, ready-to-use prompt.\n",
        "# Your response should ONLY contain the new prompt text, and nothing else.\"\"\"\n",
        "\n",
        "#     try:\n",
        "#         response = gemini_model.generate_content(reflection_prompt)\n",
        "#         return response.text.strip()\n",
        "#     except Exception as e:\n",
        "#         raise Exception(f\"Gemini API Error: {str(e)}. Check your Gemini API Key.\")\n",
        "\n",
        "\n",
        "# def select_candidate_for_mutation(candidate_pool, num_tasks):\n",
        "#     \"\"\"Selects the next candidate to mutate based on the Pareto-based strategy.\"\"\"\n",
        "#     if len(candidate_pool) == 1:\n",
        "#         return candidate_pool[0]\n",
        "\n",
        "#     best_scores_per_task = [-1.0] * num_tasks\n",
        "#     for candidate in candidate_pool:\n",
        "#         for i in range(num_tasks):\n",
        "#             if candidate[\"scores\"][i] > best_scores_per_task[i]:\n",
        "#                 best_scores_per_task[i] = candidate[\"scores\"][i]\n",
        "\n",
        "#     pareto_front_ids = set()\n",
        "#     for i in range(num_tasks):\n",
        "#         for candidate in candidate_pool:\n",
        "#             if abs(candidate[\"scores\"][i] - best_scores_per_task[i]) < 1e-6:\n",
        "#                 pareto_front_ids.add(candidate[\"id\"])\n",
        "\n",
        "#     if not pareto_front_ids:\n",
        "#         return max(candidate_pool, key=lambda c: c[\"avg_score\"])\n",
        "\n",
        "#     selected_id = random.choice(list(pareto_front_ids))\n",
        "#     return next(c for c in candidate_pool if c[\"id\"] == selected_id)\n",
        "\n",
        "\n",
        "# def test_model_connection(hf_client, model_id):\n",
        "#     \"\"\"Test if the model is accessible and working.\"\"\"\n",
        "#     test_prompt = \"Hello, world!\"\n",
        "#     try:\n",
        "#         response = run_huggingface_rollout(hf_client, model_id, \"Say hello\", test_prompt)\n",
        "#         return True, response\n",
        "#     except Exception as e:\n",
        "#         print(e)\n",
        "#         return False, str(e)\n",
        "\n",
        "\n",
        "# def run_gepa_optimization(hf_token, gemini_key, model_id, seed_prompt, training_data, budget):\n",
        "#     \"\"\"\n",
        "#     The main function that orchestrates the GEPA optimization process in Colab.\n",
        "#     \"\"\"\n",
        "#     # --- Initialization ---\n",
        "#     print(log_message(\"Starting GEPA Optimization Process...\"))\n",
        "#     hf_client = InferenceClient(token=hf_token)\n",
        "#     genai.configure(api_key=gemini_key)\n",
        "#     gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "#     rollout_count = 0\n",
        "#     candidate_pool = []\n",
        "#     best_candidate = {\"prompt\": \"Initializing...\", \"avg_score\": -1.0}\n",
        "\n",
        "#     # --- Test Model Connection ---\n",
        "#     print(log_message(f\"Testing connection to model: {model_id}\"))\n",
        "#     connection_ok, test_result = test_model_connection(hf_client, model_id)\n",
        "#     if not connection_ok:\n",
        "#         print(log_message(f\"Model connection failed: {test_result}\", 'fail'))\n",
        "#         raise Exception(f\"Cannot connect to model '{model_id}': {test_result}\")\n",
        "#     print(log_message(\"Model connection successful!\", 'success'))\n",
        "\n",
        "#     # --- Initial Evaluation of Seed Prompt ---\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(log_message(\"Phase 1: Evaluating Initial Seed Prompt\"))\n",
        "#     initial_candidate = {\"id\": 0, \"prompt\": seed_prompt, \"parentId\": None, \"scores\": [0.0] * len(training_data), \"avg_score\": 0.0}\n",
        "#     total_score = 0.0\n",
        "#     for i, task in enumerate(training_data):\n",
        "#         print(log_message(f\"  - Evaluating seed on task {i+1}/{len(training_data)}...\"))\n",
        "#         try:\n",
        "#             output = run_huggingface_rollout(hf_client, model_id, initial_candidate[\"prompt\"], task[\"input\"])\n",
        "#             eval_result = evaluation_and_feedback_function(output, task)\n",
        "#             initial_candidate[\"scores\"][i] = eval_result[\"score\"]\n",
        "#             total_score += eval_result[\"score\"]\n",
        "#         except Exception as e:\n",
        "#             print(log_message(f\"Error on task {i+1}: {str(e)}\", 'fail'))\n",
        "#             initial_candidate[\"scores\"][i] = 0.0\n",
        "#         finally:\n",
        "#              rollout_count += 1\n",
        "\n",
        "#     initial_candidate[\"avg_score\"] = total_score / len(training_data) if training_data else 0.0\n",
        "#     candidate_pool.append(initial_candidate)\n",
        "#     best_candidate = initial_candidate\n",
        "\n",
        "#     print(log_message(f\"Seed prompt initial score: {initial_candidate['avg_score']:.2f}\", 'best'))\n",
        "#     print(f\"Current Best Prompt:\\n---\\n{best_candidate['prompt']}\\n---\")\n",
        "\n",
        "\n",
        "#     # --- Main Optimization Loop ---\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(log_message(f\"Phase 2: Starting Optimization Loop (Budget: {budget} rollouts)\"))\n",
        "#     while rollout_count < budget:\n",
        "#         iteration_start_rollouts = rollout_count\n",
        "#         print(log_message(f\"--- Iteration Start (Rollouts: {rollout_count}/{budget}) ---\"))\n",
        "\n",
        "#         parent_candidate = select_candidate_for_mutation(candidate_pool, len(training_data))\n",
        "#         print(log_message(f\"Selected candidate #{parent_candidate['id']} (Score: {parent_candidate['avg_score']:.2f}) for mutation.\"))\n",
        "\n",
        "#         task_index = random.randint(0, len(training_data) - 1)\n",
        "#         reflection_task = training_data[task_index]\n",
        "#         print(log_message(f\"Performing reflective mutation using task {task_index + 1}...\"))\n",
        "\n",
        "#         try:\n",
        "#             rollout_output = run_huggingface_rollout(hf_client, model_id, parent_candidate[\"prompt\"], reflection_task[\"input\"])\n",
        "#             rollout_count += 1\n",
        "#             eval_result = evaluation_and_feedback_function(rollout_output, reflection_task)\n",
        "\n",
        "#             new_prompt = reflect_and_propose_new_prompt(gemini_model, parent_candidate[\"prompt\"], [{\n",
        "#                 \"input\": reflection_task[\"input\"], \"output\": rollout_output, \"feedback\": eval_result[\"feedback\"]\n",
        "#             }])\n",
        "\n",
        "#             new_candidate = {\"id\": len(candidate_pool), \"prompt\": new_prompt, \"parentId\": parent_candidate[\"id\"], \"scores\": [0.0] * len(training_data), \"avg_score\": 0.0}\n",
        "#             print(log_message(f\"Generated new candidate prompt #{new_candidate['id']}.\"))\n",
        "\n",
        "#             new_total_score = 0.0\n",
        "#             for i, task in enumerate(training_data):\n",
        "#                 if rollout_count >= budget:\n",
        "#                     print(log_message(\"Budget exhausted during evaluation of new candidate.\", 'fail'))\n",
        "#                     break\n",
        "#                 try:\n",
        "#                     output = run_huggingface_rollout(hf_client, model_id, new_candidate[\"prompt\"], task[\"input\"])\n",
        "#                     eval_result = evaluation_and_feedback_function(output, task)\n",
        "#                     new_candidate[\"scores\"][i] = eval_result[\"score\"]\n",
        "#                     new_total_score += eval_result[\"score\"]\n",
        "#                 except Exception as e:\n",
        "#                     print(log_message(f\"Error evaluating new candidate on task {i+1}: {str(e)}\", 'fail'))\n",
        "#                     new_candidate[\"scores\"][i] = 0.0\n",
        "#                 finally:\n",
        "#                     rollout_count += 1\n",
        "\n",
        "#             new_candidate[\"avg_score\"] = new_total_score / len(training_data) if training_data else 0.0\n",
        "\n",
        "#             if new_candidate[\"avg_score\"] > parent_candidate[\"avg_score\"]:\n",
        "#                 print(log_message(f\"New candidate #{new_candidate['id']} improved! Score: {new_candidate['avg_score']:.2f} > {parent_candidate['avg_score']:.2f}\", 'success'))\n",
        "#                 candidate_pool.append(new_candidate)\n",
        "#                 if new_candidate[\"avg_score\"] > best_candidate[\"avg_score\"]:\n",
        "#                     best_candidate = new_candidate\n",
        "#                     print(log_message(\"NEW BEST PROMPT FOUND!\", 'best'))\n",
        "#                     print(f\"Current Best Prompt:\\n---\\n{best_candidate['prompt']}\\n---\")\n",
        "#             else:\n",
        "#                 print(log_message(f\"New candidate #{new_candidate['id']} did not improve. Score: {new_candidate['avg_score']:.2f}. Discarding.\", 'fail'))\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(log_message(f\"Error in optimization iteration: {str(e)}\", 'fail'))\n",
        "#             rollout_count += 1 # Count the failed attempt if it didn't happen above\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(log_message(\"Optimization budget exhausted. Finished.\", 'best'))\n",
        "#     print(f\"Final Best Prompt (Score: {best_candidate['avg_score']:.2f}):\")\n",
        "#     print(f\"\\n{best_candidate['prompt']}\\n\")\n",
        "#     print(\"=\"*50)\n",
        "#     return best_candidate\n",
        "\n",
        "# # ==============================================================================\n",
        "# #                             MAIN EXECUTION BLOCK\n",
        "# # ==============================================================================\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # --- 2. Configuration ---\n",
        "#     # Set your parameters for the optimization run here.\n",
        "\n",
        "#     # --- API Keys (loaded from Colab Secrets) ---\n",
        "#     try:\n",
        "#         HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "#         GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "#     except Exception as e:\n",
        "#         print(\"Could not load API keys from Colab Secrets. Please ensure 'HF_TOKEN' and 'GEMINI_API_KEY' are set.\")\n",
        "#         # Terminate execution if keys are not found\n",
        "#         HF_TOKEN, GEMINI_API_KEY = None, None\n",
        "\n",
        "#     # --- Model and Prompt Settings ---\n",
        "#     # Model to optimize for. Use free-tier models like 'gpt2' or 'microsoft/DialoGPT-medium' for easy testing.\n",
        "#     # For models like Gemma, ensure you've accepted the license on its Hugging Face page.\n",
        "#     MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "\n",
        "#     SEED_PROMPT = \"You are a helpful assistant that summarizes text. Given the following text, provide a one-sentence summary that captures the main points.\"\n",
        "\n",
        "#     # --- Training Data ---\n",
        "#     # Provide a list of dictionaries. Each dictionary needs an \"input\" and the\n",
        "#     # criteria for the evaluation_and_feedback_function (e.g., \"expected_keywords\").\n",
        "#     TRAINING_DATA_JSON = \"\"\"\n",
        "#     [\n",
        "#       {\n",
        "#         \"input\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
        "#         \"expected_keywords\": [\"Eiffel Tower\", \"Paris\"]\n",
        "#       },\n",
        "#       {\n",
        "#         \"input\": \"The Great Wall of China is a series of fortifications that were built across the historical northern borders of ancient Chinese states and Imperial China as protection against various nomadic groups from the Eurasian Steppe.\",\n",
        "#         \"expected_keywords\": [\"Great Wall\", \"China\", \"fortifications\"]\n",
        "#       },\n",
        "#       {\n",
        "#         \"input\": \"The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy, just east of the Roman Forum. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age.\",\n",
        "#         \"expected_keywords\": [\"Colosseum\", \"Rome\", \"amphitheatre\"]\n",
        "#       }\n",
        "#     ]\n",
        "#     \"\"\"\n",
        "#     TRAINING_DATA = json.loads(TRAINING_DATA_JSON)\n",
        "\n",
        "#     # --- Budget ---\n",
        "#     # Total number of times the target model will be called. Start with a small number (5-10) to test.\n",
        "#     BUDGET = 10\n",
        "\n",
        "#     # --- 3. Run Optimization ---\n",
        "#     if HF_TOKEN and GEMINI_API_KEY:\n",
        "#       try:\n",
        "#           final_result = run_gepa_optimization(\n",
        "#               hf_token=HF_TOKEN,\n",
        "#               gemini_key=GEMINI_API_KEY,\n",
        "#               model_id=MODEL_ID,\n",
        "#               seed_prompt=SEED_PROMPT,\n",
        "#               training_data=TRAINING_DATA,\n",
        "#               budget=BUDGET\n",
        "#           )\n",
        "#       except Exception as e:\n",
        "#           print(f\"\\nAn unrecoverable error occurred during execution: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjVyRvCSd1QT"
      },
      "source": [
        "## Use Google endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AyJ_8MD2sM1"
      },
      "source": [
        "#### LLM as Judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "hsBDj6qMRFwN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def call_with_backoff(func, max_retries=5, *args, **kwargs):\n",
        "    delay = 0.1  # initial delay\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            if \"rate limit\" in str(e).lower() or \"429\" in str(e):  # customize based on error\n",
        "                print(f\"Rate limit hit, backing off... attempt {attempt+1}\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2  # exponential backoff\n",
        "                delay += random.uniform(0, 0.01)  # jitter\n",
        "            else:\n",
        "                raise e\n",
        "    raise RuntimeError(\"Exceeded maximum retries due to rate limiting.\")\n",
        "\n",
        "    # print(\"============================\")\n",
        "    # print(f\"QUESTION: {question}\")\n",
        "    # print(\"----------------------------------\")\n",
        "    # answer_score_pairs = [(response, normalized_scores[i]) for i, response in enumerate(responses)]\n",
        "    # for answer, score in answer_score_pairs:\n",
        "    #   print(f\"Answer: {answer} Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "xBDbBgp89v65"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "from litellm import completion\n",
        "from pydantic import BaseModel, Field, confloat\n",
        "from typing import List\n",
        "\n",
        "class TrajectoryScore(BaseModel):\n",
        "    feedback: str\n",
        "    score: confloat(ge=-10.0, le=100.0)\n",
        "\n",
        "# class TrajectoryGradingOutput(BaseModel):\n",
        "#     scores: List[TrajectoryScore]\n",
        "\n",
        "def get_gemini_completion(judge_model_name, rubric, question, responses, answer, size_group) -> TrajectoryScore:\n",
        "    judge_prompt = f\"\"\"\n",
        "You are a grader evaluating an agent response against a goal rubric.\n",
        "Give the trajectory a score between -10 and 100 and a quick feedback (less than 150 chars) on the trajectory's performance, if\n",
        "the answer is numerically correct and if the formatting was completely or partially followed.\n",
        "\n",
        "Rubric:\n",
        "{rubric}\n",
        "\n",
        "Prompt:\n",
        "\n",
        "{question}\n",
        "\n",
        "Responses:\n",
        "{responses}\n",
        "\n",
        "Right answer: {answer}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = completion(\n",
        "            model=judge_model_name,  # LiteLLM-style model name for Gemini\n",
        "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
        "            response_format=TrajectoryScore,  # Enforce Pydantic format\n",
        "            max_retries=2,  # Optional: retry if model returns malformed output,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        return response  # this will be a parsed `TrajectoryGradingOutput` instance\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Validation or generation error: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9vUIXRu20aA"
      },
      "source": [
        "#### Run rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "gO5feCsJ0bAQ"
      },
      "outputs": [],
      "source": [
        "# --- 1. Installation and Imports ---\n",
        "# Install necessary libraries in the Colab environment\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "#from google.colab import userdata # For securely accessing API keys\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def test_model_connection(target_model):\n",
        "    \"\"\"Test if the model is accessible and working via the Google AI API.\"\"\"\n",
        "    test_prompt = \"Say hello\"\n",
        "    test_input = \"Hello, world!\"\n",
        "    try:\n",
        "        response = run_google_rollout(target_model, test_prompt, test_input)\n",
        "        return True, response\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "def log_message(message, type='info'):\n",
        "    \"\"\"Helper to format log messages with a timestamp.\"\"\"\n",
        "    timestamp = time.strftime(\"%H:%M:%S\")\n",
        "    if type == 'success':\n",
        "        return f\"[{timestamp}] ✅ SUCCESS: {message}\"\n",
        "    if type == 'fail':\n",
        "        return f\"[{timestamp}] ❌ FAIL: {message}\"\n",
        "    if type == 'best':\n",
        "        return f\"[{timestamp}] ⭐ BEST: {message}\"\n",
        "    return f\"[{timestamp}] ℹ️ INFO: {message}\"\n",
        "\n",
        "# ------ Ollama Implementation ------\n",
        "import requests\n",
        "\n",
        "def run_ollama_rollout(target_model, system_prompt, question):\n",
        "    \"\"\"\n",
        "    Calls Ollama for the target model.\n",
        "    This function performs a \"rollout\" for a given prompt and input.\n",
        "    \"\"\"\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "    messages = [{\"content\": system_prompt, \"role\":\"system\"}, {\"content\": question, \"role\":\"user\"}]\n",
        "    data = {\n",
        "        \"model\": target_model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"num_predict\": 800,  # this is Ollama's version of max_tokens,\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Core GEPA Functions (Google API Implementation) ---\n",
        "\n",
        "def run_google_rollout(target_model, prompt, input_text):\n",
        "    \"\"\"\n",
        "    Calls the Google Generative AI API for the target model.\n",
        "    This function performs a \"rollout\" for a given prompt and input.\n",
        "    \"\"\"\n",
        "    full_prompt = f\"{prompt}\\n\\nText: \\\"{input_text}\\\"\\n\\nResponse:\"\n",
        "    generation_config = genai.types.GenerationConfig(\n",
        "        max_output_tokens=600,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    try:\n",
        "        response = target_model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        # Handle cases where the model response might be empty or blocked\n",
        "        if not response.parts:\n",
        "            raise Exception(\"Model returned an empty response. This could be due to safety filters.\")\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        # Provide a more specific error message for API key issues\n",
        "        if \"api_key\" in str(e).lower():\n",
        "            raise Exception(f\"Google AI API Error: Authorization failed. Ensure your Gemini API Key is correct and enabled.\")\n",
        "        raise Exception(f\"Google AI API Error during rollout: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ54TJkj24mF"
      },
      "source": [
        "#### Reflective Prompt Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "VGQDhl7K1A4i"
      },
      "outputs": [],
      "source": [
        "def reflect_and_propose_new_prompt(reflector_model, current_prompt, examples):\n",
        "    \"\"\"\n",
        "    Performs the Reflective Prompt Mutation step using a powerful LLM (e.g., Gemini Pro).\n",
        "    \"\"\"\n",
        "    examples_text = '---'.join(\n",
        "        f'Task Input: \"{e[\"input\"]}\"\\nGenerated Output: \"{e[\"output\"]}\"\\nFeedback:\\n{e[\"feedback\"]}\\n\\n'\n",
        "        for e in examples\n",
        "    )\n",
        "\n",
        "    reflection_prompt = f\"\"\"You are an expert prompt engineer. Your task is to refine a prompt to improve its performance based on feedback from previous attempts.\n",
        "\n",
        "Here is the current prompt that needs improvement:\n",
        "--- CURRENT PROMPT ---\n",
        "{current_prompt}\n",
        "--------------------\n",
        "\n",
        "Here are examples of how the prompt performed on a few tasks, along with feedback on what went wrong or right:\n",
        "--- EXAMPLES & FEEDBACK ---\n",
        "{examples_text}\n",
        "-------------------------\n",
        "\n",
        "Based on this analysis, your task is to write a new, improved prompt. The new prompt should be a complete set of instructions that directly addresses the failures and incorporates the successful strategies observed in the feedback. Do not just give suggestions; provide the full, ready-to-use prompt.\n",
        "Your response should ONLY contain the new prompt text, and nothing else.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = reflector_model.generate_content(reflection_prompt)\n",
        "        if not response.parts:\n",
        "             raise Exception(\"Reflector model returned an empty response. This could be due to safety filters.\")\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Gemini API Error during reflection: {str(e)}. Check your Gemini API Key.\")\n",
        "\n",
        "\n",
        "def select_candidate_for_mutation(candidate_pool, num_tasks):\n",
        "    \"\"\"Selects the next candidate to mutate based on the Pareto-based strategy.\"\"\"\n",
        "    if len(candidate_pool) == 1:\n",
        "        return candidate_pool[0]\n",
        "\n",
        "    best_scores_per_task = [-1.0] * num_tasks\n",
        "    for candidate in candidate_pool:\n",
        "        for i in range(num_tasks):\n",
        "            if candidate[\"scores\"][i] > best_scores_per_task[i]:\n",
        "                best_scores_per_task[i] = candidate[\"scores\"][i]\n",
        "\n",
        "    pareto_front_ids = set()\n",
        "    for i in range(num_tasks):\n",
        "        for candidate in candidate_pool:\n",
        "            if abs(candidate[\"scores\"][i] - best_scores_per_task[i]) < 1e-6:\n",
        "                pareto_front_ids.add(candidate[\"id\"])\n",
        "\n",
        "    if not pareto_front_ids:\n",
        "        return max(candidate_pool, key=lambda c: c[\"avg_score\"])\n",
        "\n",
        "    selected_id = random.choice(list(pareto_front_ids))\n",
        "    return next(c for c in candidate_pool if c[\"id\"] == selected_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcVTLjg-1EDf"
      },
      "source": [
        "#### Reward Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Eiu614bGQ1OI",
        "outputId": "21143b39-c0ad-4ff0-abc2-84e975d8a77d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "kE-7-h22OU9e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\\\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "\n",
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "\n",
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    feedback = \"\"\n",
        "    for response in completions:\n",
        "        score = 0\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None:\n",
        "          score += 3.0\n",
        "          feedback = \"Formatting correct\"\n",
        "        else:\n",
        "          score -= 3.0\n",
        "          feedback = \"Formatting missing\"\n",
        "        scores.append(score)\n",
        "    return scores, feedback\n",
        "\n",
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    feedback = []\n",
        "    for response in completions:\n",
        "        score = 0\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "        if response.count(reasoning_start) == 1:\n",
        "          score += 0.5\n",
        "          feedback.append(f\"{reasoning_start} present. \")\n",
        "        else:\n",
        "          score -= 0.5\n",
        "          feedback.append(f\"{reasoning_start} missing. \")\n",
        "\n",
        "        if response.count(reasoning_end)   == 1:\n",
        "          score += 0.5\n",
        "          feedback.append(f\"{reasoning_end} present. \")\n",
        "        else:\n",
        "          score -= 0.5\n",
        "          feedback.append(f\"{reasoning_end} missing. \")\n",
        "\n",
        "        if response.count(solution_start)  == 1:\n",
        "          score += 0.5\n",
        "          feedback.append(f\"{solution_start} present. \")\n",
        "        else:\n",
        "          score -= 0.5\n",
        "          feedback.append(f\"{solution_start} missing. \")\n",
        "\n",
        "        if response.count(solution_end)    == 1:\n",
        "          score += 0.5\n",
        "          feedback.append(f\"{solution_end} present. \")\n",
        "        else:\n",
        "          score -= 0.5\n",
        "          feedback.append(f\"{solution_end} missing. \")\n",
        "\n",
        "        scores.append(score)\n",
        "    return scores, \"\".join(feedback)\n",
        "\n",
        "def check_answer(completions, answer, **kwargs):\n",
        "    responses = [completion for completion in completions]\n",
        "    feedback = \"\"\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Correct answer gets 3 points!\n",
        "        if guess == true_answer:\n",
        "            score += 3.0\n",
        "            feedback = \" Answer is correct! \"\n",
        "        # Match if spaces are seen\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 1.5\n",
        "            feedback = \"Answer is correct but there are trailing spaces. \"\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
        "                else: score -= 1.0 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 0.5 # Penalize\n",
        "            feedback = \"Wrong answer! \"\n",
        "        scores.append(score)\n",
        "    return scores, feedback\n",
        "\n",
        "def check_numbers(completions, answer, **kwargs):\n",
        "    responses = [completion for completion in completions]\n",
        "    feedback = \"\"\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            feedback = \" No answer provided! \"\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            guess       = float(guess.strip())\n",
        "            if guess == true_answer:\n",
        "              scores.append(1.5)\n",
        "              feedback = \"Right answer.\"\n",
        "            else:\n",
        "              scores.append(0)\n",
        "              feedback = \"Wrong answer.\"\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores, feedback\n",
        "\n",
        "def evaluation_and_feedback_function(output, answer, *kargs):\n",
        "  score_format_exactly, feedback_format_exactly = match_format_exactly([output])\n",
        "  score_format_approximately, feedback_format_approximately = match_format_approximately([output])\n",
        "  score_check_answer, feedback_check_answer = check_answer([output], [answer])\n",
        "  score_check_numbers, feedback_check_numbers = check_numbers([output], [answer])\n",
        "  #print(f\"score_format_exactly: {score_format_exactly}\")\n",
        "  #print(f\"score_format_approximately: {score_format_approximately}\")\n",
        "  #print(f\"score_check_numbers: {score_check_numbers}\")\n",
        "  return {\n",
        "    \"score\": score_format_exactly[0] +\n",
        "    score_format_approximately[0] +\n",
        "    score_check_answer[0] +\n",
        "    score_check_numbers[0],\n",
        "    \"feedback\": \", \".join([feedback_format_exactly,\n",
        "                           feedback_format_approximately,\n",
        "                           feedback_check_answer,\n",
        "                           feedback_check_numbers])\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkcrPDBfQD0s",
        "outputId": "1871c8a3-aed2-459b-81c6-64ae5258d087"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 9.5,\n",
              " 'feedback': 'Formatting correct, <start_working_out> present. <end_working_out> present. <SOLUTION> present. </SOLUTION> present. ,  Answer is correct! , Right answer.'}"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#test eval function\n",
        "response = \"\"\"<start_working_out>\n",
        "<end_working_out>\n",
        "<SOLUTION>12</SOLUTION>\"\"\"\n",
        "evaluation_and_feedback_function(response, \"12\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "kZWemw6E3FCH"
      },
      "outputs": [],
      "source": [
        "# def evaluation_and_feedback_function(output, task, judge_model_name):\n",
        "#   question = task['prompt'][-1][\"content\"]\n",
        "#   if not output or not isinstance(output, str):\n",
        "#     return {\"score\": 0.0, \"feedback\": \"No valid output generated.\"}\n",
        "#   #print(output)\n",
        "#   predicted_answer = extract_number(output)\n",
        "#   expected_answer = task.get(\"answer\", \"\")\n",
        "\n",
        "#   rubric = \"\"\"Consider in this order of relevance to give a score:\n",
        "#   1 - Accuracy of the answer (if the answer matches exactly the expected)\n",
        "#   2- If the right answers appears somewhere between <SOLUTION></SOLUTION> give some points.\n",
        "#   3- Less important, answer should follow Formatting: <start_working_out>.+?{stop_working_out}.*? and {<SOLUTION>}(.+?){</SOLUTION>}\n",
        "\n",
        "#   If answer is correct and only the number is between <SOLUTION></SOLUTION> maximum grade.\n",
        "#   If answer is incorrect but there's a number between <SOLUTION></SOLUTION> give low grades proportional to how close the answer is to the right one.\n",
        "#   If neither, negative score.\n",
        "#   \"\"\"\n",
        "\n",
        "#   response = call_with_backoff(\n",
        "#   get_gemini_completion,\n",
        "#   max_retries=2,\n",
        "#   judge_model_name=judge_model_name,\n",
        "#   rubric=rubric,\n",
        "#   question=question,\n",
        "#   responses=output,\n",
        "#   answer=expected_answer,\n",
        "#   size_group=1\n",
        "# )\n",
        "#   first_choice = response.choices[0]\n",
        "#   content = first_choice.message.content or \"{}\"\n",
        "#   is_answer_correct = 1 if predicted_answer == expected_answer else 0\n",
        "#   print(\"===================== content ===================\")\n",
        "#   print(f\"predicted: {predicted_answer} expected: {expected_answer}\")\n",
        "#   try:\n",
        "#     parsed_results = TrajectoryScore.model_validate_json(content) #parse_litellm_json_response(response)\n",
        "#     return {\"score\":10*is_answer_correct + parsed_results.score, \"feedback\":parsed_results.feedback}\n",
        "#   except Exception as e:\n",
        "#     print(f\"JSON parsing failed: {e}\")\n",
        "#     return {\"score\": 0, \"feedback\": \"incorrect answer\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "j7VO-BQK1GHS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def evaluation_and_feedback_function(output, task):\n",
        "#   if not output or not isinstance(output, str):\n",
        "#     return {\"score\": 0.0, \"feedback\": \"No valid output generated.\"}\n",
        "#   print(output)\n",
        "#   predicted_answer = extract_number(output)\n",
        "#   expected_answer = task.get(\"answer\", [])\n",
        "\n",
        "#   if not predicted_answer:\n",
        "#     return {\"score\": 0.0, \"feedback\": \"No answer found\"}\n",
        "\n",
        "#   if predicted_answer == expected_answer:\n",
        "#     return {\"score\": 1, \"feedback\": \"correct answer\"}\n",
        "#   else:\n",
        "#     return {\"score\": 0, \"feedback\": \"incorrect answer\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def evaluation_and_feedback_function(output, task):\n",
        "#     \"\"\"\n",
        "#     The evaluation function (μ_f in the paper). Scores the model's output\n",
        "#     and provides textual feedback.\n",
        "#     IMPORTANT: This is the most critical part to customize for a specific task.\n",
        "#     \"\"\"\n",
        "#     if not output or not isinstance(output, str):\n",
        "#         return {\"score\": 0.0, \"feedback\": \"No valid output generated.\"}\n",
        "\n",
        "#     # --- CUSTOMIZE THIS FUNCTION ---\n",
        "#     # This example checks for keyword presence. For a real task, you might use\n",
        "#     # regex, semantic similarity, code compilation, etc.\n",
        "#     score = 0.0\n",
        "#     feedback = \"\"\n",
        "#     found_keywords = 0\n",
        "#     expected_keywords = task.get(\"answer\", [])\n",
        "\n",
        "#     if not expected_keywords:\n",
        "#         return {\"score\": 0.0, \"feedback\": \"No evaluation criteria (expected_keywords) found in training data.\"}\n",
        "\n",
        "#     for keyword in expected_keywords:\n",
        "#         if keyword.lower() in output.lower():\n",
        "#             found_keywords += 1\n",
        "#             feedback += f\"SUCCESS: Output correctly contained '{keyword}'.\\n\"\n",
        "#         else:\n",
        "#             feedback += f\"FAILURE: Output was missing required keyword '{keyword}'.\\n\"\n",
        "\n",
        "#     score = found_keywords / len(expected_keywords) if expected_keywords else 0.0\n",
        "#     feedback += f\"Final Score for this task: {score:.2f}\"\n",
        "#     return {\"score\": score, \"feedback\": feedback}\n",
        "#     # --- END CUSTOMIZATION ---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mini batch generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_mini_batch(dataset, size_batch=3):\n",
        "    rng = np.random.default_rng()\n",
        "    sampled_indices = rng.choice(len(dataset), size=size_batch, replace=False)\n",
        "\n",
        "    # Get the sampled dataset\n",
        "    mini_batch = dataset.select(sampled_indices)\n",
        "\n",
        "    return sampled_indices, mini_batch\n",
        "\n",
        "def mini_batch_rollout(target_model, prompt, mini_batch):\n",
        "    total_score = 0\n",
        "    feedback_list = []\n",
        "    completions = []\n",
        "    for i, example in enumerate(mini_batch):\n",
        "        feedback_list.append(f\"------------ Feedback for question {i} ------------------\")\n",
        "        completion = run_ollama_rollout(target_model, prompt, example['question'])\n",
        "        completions.append(completion)\n",
        "        eval_result = evaluation_and_feedback_function(completion, example['answer'])\n",
        "        total_score += eval_result['score']\n",
        "        feedback_list.append(eval_result['feedback'])\n",
        "    \n",
        "    return \".New answer: \".join(completions), {\"score\": total_score/len(mini_batch), \"feedback\": \"\".join(feedback_list)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhY98aq61G7x"
      },
      "source": [
        "#### GEPA Optmization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "nb94SyV0Lds8"
      },
      "outputs": [],
      "source": [
        "def run_gepa_optimization(target_model_name, reflector_model_name, judge_model_name, seed_prompt, training_data, budget):\n",
        "    \"\"\"\n",
        "    The main function that orchestrates the GEPA optimization process.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    print(log_message(\"Starting GEPA Optimization Process...\"))\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    target_model = target_model_name #genai.GenerativeModel(target_model_name)\n",
        "    reflector_model = genai.GenerativeModel(reflector_model_name)\n",
        "\n",
        "    rollout_count = 0\n",
        "    candidate_pool = []\n",
        "    best_candidate = {\"prompt\": \"Initializing...\", \"avg_score\": -1.0}\n",
        "\n",
        "    # --- Test Model Connection ---\n",
        "    #print(log_message(f\"Testing connection to target model: {target_model_name}\"))\n",
        "    #connection_ok, test_result = test_model_connection(target_model)\n",
        "    #if not connection_ok:\n",
        "    #    print(log_message(f\"Model connection failed: {test_result}\", 'fail'))\n",
        "    #    raise Exception(f\"Cannot connect to model '{target_model_name}': {test_result}\")\n",
        "    #print(log_message(\"Model connection successful!\", 'success'))\n",
        "\n",
        "    # --- Initial Evaluation of Seed Prompt ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(log_message(\"Phase 1: Evaluating Initial Seed Prompt\"))\n",
        "    initial_candidate = {\"id\": 0, \"prompt\": seed_prompt, \"parentId\": None, \"scores\": [0.0] * len(training_data), \"avg_score\": 0.0}\n",
        "    total_score = 0.0\n",
        "    for i, task in enumerate(training_data):\n",
        "        print(log_message(f\"  - Evaluating seed on task {i+1}/{len(training_data)}...\"))\n",
        "        try:\n",
        "            output = run_ollama_rollout(target_model, initial_candidate[\"prompt\"], task[\"question\"])\n",
        "            eval_result = evaluation_and_feedback_function(output, task['answer'], judge_model_name)\n",
        "            initial_candidate[\"scores\"][i] = eval_result[\"score\"]\n",
        "            total_score += eval_result[\"score\"]\n",
        "        except Exception as e:\n",
        "            print(log_message(f\"Error on task {i+1}: {str(e)}\", 'fail'))\n",
        "            initial_candidate[\"scores\"][i] = 0.0\n",
        "        finally:\n",
        "            rollout_count += 1\n",
        "\n",
        "    initial_candidate[\"avg_score\"] = total_score / len(training_data) if training_data else 0.0\n",
        "    candidate_pool.append(initial_candidate)\n",
        "    best_candidate = initial_candidate\n",
        "\n",
        "    print(log_message(f\"Seed prompt initial score: {initial_candidate['avg_score']:.2f}\", 'best'))\n",
        "    print(f\"Current Best Prompt:\\n---\\n{best_candidate['prompt']}\\n---\")\n",
        "\n",
        "\n",
        "    # --- Main Optimization Loop ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(log_message(f\"Phase 2: Starting Optimization Loop (Budget: {budget} rollouts)\"))\n",
        "    while rollout_count < budget:\n",
        "        iteration_start_rollouts = rollout_count\n",
        "        print(log_message(f\"--- Iteration Start (Rollouts: {rollout_count}/{budget}) ---\"))\n",
        "\n",
        "        parent_candidate = select_candidate_for_mutation(candidate_pool, len(training_data))\n",
        "        print(log_message(f\"Selected candidate #{parent_candidate['id']} (Score: {parent_candidate['avg_score']:.2f}) for mutation.\"))\n",
        "\n",
        "        #task_index = random.randint(0, len(training_data) - 1)\n",
        "        #reflection_task = training_data[task_index]\n",
        "        indexes, mini_batch = generate_mini_batch(training_data)\n",
        "        questions = [f\" Question {i}: {question} \" for i, question in enumerate(mini_batch['question'])]\n",
        "        print(log_message(f\"Performing reflective mutation using indices {indexes}...\"))\n",
        "\n",
        "        try:\n",
        "            #rollout_output = run_ollama_rollout(target_model, parent_candidate[\"prompt\"], reflection_task[\"question\"])\n",
        "            rollout_count += 1\n",
        "            rollouts, eval_result = mini_batch_rollout(target_model, initial_candidate[\"prompt\"], mini_batch)\n",
        "            #eval_result = evaluation_and_feedback_function(rollout_output, reflection_task, judge_model_name)\n",
        "\n",
        "            new_prompt = reflect_and_propose_new_prompt(reflector_model, parent_candidate[\"prompt\"], [{\n",
        "                \"input\": \"\".join(questions), \"output\": rollouts, \"feedback\": eval_result[\"feedback\"]\n",
        "            }])\n",
        "\n",
        "            new_candidate = {\"id\": len(candidate_pool), \"prompt\": new_prompt, \"parentId\": parent_candidate[\"id\"], \"scores\": [0.0] * len(training_data), \"avg_score\": 0.0}\n",
        "            print(log_message(f\"Generated new candidate prompt #{new_candidate['id']}.\"))\n",
        "\n",
        "            _, new_prompt_eval_result = mini_batch_rollout(target_model, new_candidate[\"prompt\"], mini_batch)\n",
        "\n",
        "            #if performance in mini batch is not improved discard mutated prompt\n",
        "            if new_prompt_eval_result['score'] < eval_result['score']:\n",
        "                continue\n",
        "\n",
        "            new_total_score = 0.0\n",
        "            for i, task in enumerate(training_data):\n",
        "                if rollout_count >= budget:\n",
        "                    print(log_message(\"Budget exhausted during evaluation of new candidate.\", 'fail'))\n",
        "                    break\n",
        "                try:\n",
        "                    output = run_ollama_rollout(target_model, new_candidate[\"prompt\"], task[\"question\"])\n",
        "                    eval_result = evaluation_and_feedback_function(output, task['answer'], judge_model_name)\n",
        "                    new_candidate[\"scores\"][i] = eval_result[\"score\"]\n",
        "                    new_total_score += eval_result[\"score\"]\n",
        "                except Exception as e:\n",
        "                    print(log_message(f\"Error evaluating new candidate on task {i+1}: {str(e)}\", 'fail'))\n",
        "                    new_candidate[\"scores\"][i] = 0.0\n",
        "                finally:\n",
        "                    rollout_count += 1\n",
        "\n",
        "            new_candidate[\"avg_score\"] = new_total_score / len(training_data) if training_data else 0.0\n",
        "\n",
        "            if new_candidate[\"avg_score\"] > parent_candidate[\"avg_score\"]:\n",
        "                print(log_message(f\"New candidate #{new_candidate['id']} improved! Score: {new_candidate['avg_score']:.2f} > {parent_candidate['avg_score']:.2f}\", 'success'))\n",
        "                candidate_pool.append(new_candidate)\n",
        "                if new_candidate[\"avg_score\"] > best_candidate[\"avg_score\"]:\n",
        "                    best_candidate = new_candidate\n",
        "                    print(log_message(\"NEW BEST PROMPT FOUND!\", 'best'))\n",
        "                    print(f\"Current Best Prompt:\\n---\\n{best_candidate['prompt']}\\n---\")\n",
        "            else:\n",
        "                print(log_message(f\"New candidate #{new_candidate['id']} did not improve. Score: {new_candidate['avg_score']:.2f}. Discarding.\", 'fail'))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(log_message(f\"Error in optimization iteration: {str(e)}\", 'fail'))\n",
        "            # Ensure rollout is counted even if a step fails before the evaluation loop\n",
        "            if iteration_start_rollouts == rollout_count:\n",
        "                rollout_count += 1\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(log_message(\"Optimization budget exhausted. Finished.\", 'best'))\n",
        "    print(f\"Final Best Prompt (Score: {best_candidate['avg_score']:.2f}):\")\n",
        "    print(f\"\\n{best_candidate['prompt']}\\n\")\n",
        "    print(\"=\"*50)\n",
        "    return best_candidate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V9BQ3H0k8FD"
      },
      "source": [
        "#### Load train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "328830278211431c94433d19db84ea26",
            "de118a428e9b40e691f14e7dd0f8b788",
            "cb155afca1254e9891be7a28404335ac",
            "e0cda2a187d84b1f8daf507506676cce",
            "3784da7cc97f4c3f8f3cf81b8b69b952",
            "bac5c9e89e664998b92fde78ae3c8f19",
            "0926d91ff031423f806c7a780c72a46b",
            "ac91d346d14f45d99d956aebcb0b4edf",
            "e21d2e3f073e424491be2b929b913595",
            "a082d1c66b8f40c2adf2532e8f40d4a8",
            "a3612d8e705e435eb5b6f8c1f405b761",
            "c25481c025144f46b79d9c7914ad3437",
            "24dd2625515a4d9298ee9ef25dddd770",
            "91b64215b8e046dfbd0e05752d4cde44",
            "61269fae0ce84eaf9d328adcee88e2d6",
            "e76db0522da7478ab3361386fd5b325e",
            "89ece5e3eae0476d82793ed3eb98d790",
            "776470e9309542cfb5fd106273d063fd",
            "4f9e725649ff45e9a9bf256a3d844200",
            "f26ec3bca385451a81dddc3d969c778e",
            "1f985985947d4d01aedd13e87be37e5f",
            "8f8e23c5d2fe4e5ba703b7d175474987",
            "5f1d9c7ff9f1404689420ac1a949e925",
            "630a582e4d11424c9958ff2f326f40dd",
            "bfde2c7bacc84f77a5a98aaf6938c59b",
            "4f54ca8a518741e5a6098ce148d423c8",
            "bd1ef41e403440d78e6ac16967c58f63",
            "6802a8b8e92a44e18bcc910bf82bf1c2",
            "0f709f4a905e4b489af7a5b2ff0eeccf",
            "66fc377156c842009f48e57a8ac8ebcc",
            "0435c54eeedc4d78848168d4361fa0b9",
            "076afdc14205410ca5d4356b377617f7",
            "c811bf1a488c4c2084d178fc76fe8395",
            "c64784236a554b5fb826b805dd5f9619",
            "e3ddb64a60ec44d18eadf475b66bcaaf",
            "2bcfeb5366fa45e083658dc2ffb6958e",
            "8b1e42fc0ea4459c965fbbbb270e78b1",
            "3242ea99646e4302823a2027c37d4dd5",
            "26a4c7f5211841d7aac1585029232046",
            "5f35ff9a843b4a7ebb7a23a4ffa6ef6b",
            "bf86bb2a2a0e45eebbc532bab1d70134",
            "5f9dd38a164c462c9eaa536662c9ec8a",
            "9cef0abee0044328bce616e59fbcf0ba",
            "01f1a15859a540719647f8c8af2eed32",
            "47c17bf31e8e4a7db092ce103d7824d3",
            "b9659dd5cb384458a172f7e16ecc8f6a",
            "bc24f467ddff4bdabe34702f96c73bef",
            "57d170be10f64bfa8cd05bca31ffe72f",
            "933c9f3d5ba8407c80a9b2c25d6cd1cf",
            "6fe54b3c0dda432496ed8eaf51a73ea0",
            "330880f9f74c42b69bd33f7762578bf0",
            "4487086b4dd1483b9e8b27969037e2b9",
            "97f7c4c1dfa741b8af95137b445bee4e",
            "1af4bc0256ac4b12a52afcafc535e331",
            "130985d7644a4bbebd4b4139ee948dcd"
          ]
        },
        "id": "aMWGEh4Pk72V",
        "outputId": "ab447bc8-19a7-4450-cb43-88ccfc14f33a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer'],\n",
              "    num_rows: 7473\n",
              "})"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CPqZXQM8loS8",
        "outputId": "3cd46850-df87-44ee-8be1-3e40235b00fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'72'"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def extract_hash_answer(text):\n",
        "    if \"####\" not in text: return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "extract_hash_answer(dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "f9275585c532418ca02c36369ce11e1b",
            "0a2afd46122a4c20a78eebe6dbe75c96",
            "fb1c0950d7454cd38ddc0b195cafff03",
            "d5200e65ebc94e669274f82742afa955",
            "d31a75a1d4de4adc9e79f682f1c5dee9",
            "116774d7b24b4e2da5dbe87528d6c732",
            "e49e4b371e584f4f972f295fe4545dfb",
            "8c33bc1304cc4e4f883543618c0d0b6e",
            "3a785b397c8e46a4800d44039db635ce",
            "2c533bdeb492456b96e30bcedda608bd",
            "f6d659b6d3334f369d13e2586f4138ac"
          ]
        },
        "id": "Z5x32Psils3p",
        "outputId": "d8ebcbe1-1751-45ba-fe4b-9046f9a2ac42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              " 'answer': '72',\n",
              " 'prompt': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'}"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : system_prompt,\n",
        "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "M-929QoomWk0"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.select(range(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw7t2C5FCMsq",
        "outputId": "cbea19c7-cd6c-49a0-cf1c-29e7a03426e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              " 'answer': '72',\n",
              " 'prompt': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'}"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvP5A5CY3Liz"
      },
      "source": [
        "#### Run GEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aLKgSyKoks4r",
        "outputId": "65434d71-742e-4426-8221-36bd5eca9e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18:06:07] ℹ️ INFO: Starting GEPA Optimization Process...\n",
            "\n",
            "==================================================\n",
            "[18:06:07] ℹ️ INFO: Phase 1: Evaluating Initial Seed Prompt\n",
            "[18:06:07] ℹ️ INFO:   - Evaluating seed on task 1/10...\n",
            "[18:06:08] ℹ️ INFO:   - Evaluating seed on task 2/10...\n",
            "[18:06:10] ℹ️ INFO:   - Evaluating seed on task 3/10...\n",
            "[18:06:12] ℹ️ INFO:   - Evaluating seed on task 4/10...\n",
            "[18:06:14] ℹ️ INFO:   - Evaluating seed on task 5/10...\n",
            "[18:06:17] ℹ️ INFO:   - Evaluating seed on task 6/10...\n",
            "[18:06:20] ℹ️ INFO:   - Evaluating seed on task 7/10...\n",
            "[18:06:22] ℹ️ INFO:   - Evaluating seed on task 8/10...\n",
            "[18:06:23] ℹ️ INFO:   - Evaluating seed on task 9/10...\n",
            "[18:06:25] ℹ️ INFO:   - Evaluating seed on task 10/10...\n",
            "[18:06:29] ⭐ BEST: Seed prompt initial score: -2.25\n",
            "Current Best Prompt:\n",
            "---\n",
            "You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <start_working_out> and <end_working_out>.\n",
            "Then, provide your solution between <SOLUTION></SOLUTION>\n",
            "---\n",
            "\n",
            "==================================================\n",
            "[18:06:29] ℹ️ INFO: Phase 2: Starting Optimization Loop (Budget: 1000 rollouts)\n",
            "[18:06:29] ℹ️ INFO: --- Iteration Start (Rollouts: 10/1000) ---\n",
            "[18:06:29] ℹ️ INFO: Selected candidate #0 (Score: -2.25) for mutation.\n",
            "[18:06:29] ℹ️ INFO: Performing reflective mutation using indices [2 9 1]...\n",
            "[18:06:37] ℹ️ INFO: Generated new candidate prompt #1.\n",
            "[18:07:13] ✅ SUCCESS: New candidate #1 improved! Score: -2.05 > -2.25\n",
            "[18:07:13] ⭐ BEST: NEW BEST PROMPT FOUND!\n",
            "Current Best Prompt:\n",
            "---\n",
            "You are a math problem solver. Your task is to solve the given math problem step-by-step, clearly showing your work and arriving at the correct solution.\n",
            "\n",
            "1.  Carefully read and understand the problem.\n",
            "2.  Plan your approach to solve the problem.\n",
            "3.  Show all your working out within the `<start_working_out>` and `<end_working_out>` tags.\n",
            "4.  Provide your final answer within the `<SOLUTION>` and `</SOLUTION>` tags.\n",
            "\n",
            "Here is the format you MUST follow:\n",
            "\n",
            "```\n",
            "<start_working_out>\n",
            "[Your step-by-step working out here]\n",
            "</start_working_out>\n",
            "<SOLUTION>\n",
            "[Your final answer here]\n",
            "</SOLUTION>\n",
            "```\n",
            "---\n",
            "[18:07:13] ℹ️ INFO: --- Iteration Start (Rollouts: 21/1000) ---\n",
            "[18:07:13] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:07:13] ℹ️ INFO: Performing reflective mutation using indices [1 4 3]...\n",
            "[18:07:20] ℹ️ INFO: Generated new candidate prompt #2.\n",
            "[18:07:29] ℹ️ INFO: --- Iteration Start (Rollouts: 22/1000) ---\n",
            "[18:07:29] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:07:29] ℹ️ INFO: Performing reflective mutation using indices [0 8 6]...\n",
            "[18:07:43] ℹ️ INFO: Generated new candidate prompt #2.\n",
            "[18:08:12] ✅ SUCCESS: New candidate #2 improved! Score: -1.10 > -2.05\n",
            "[18:08:12] ⭐ BEST: NEW BEST PROMPT FOUND!\n",
            "Current Best Prompt:\n",
            "---\n",
            "You are a math problem solver. Your task is to solve the given math problem step-by-step, clearly showing your work and arriving at the correct solution.\n",
            "\n",
            "1.  Carefully read and understand the problem.\n",
            "2.  Plan your approach to solve the problem.\n",
            "3.  Show all your working out within the `<start_working_out>` and `<end_working_out>` tags. Ensure all steps are clearly presented.\n",
            "4.  Provide your final answer within the `<SOLUTION>` and `</SOLUTION>` tags.  The answer should be a single numerical value or a clear, concise answer to the question.\n",
            "\n",
            "Here is the format you MUST follow:\n",
            "\n",
            "```\n",
            "<start_working_out>\n",
            "[Your step-by-step working out here]\n",
            "</start_working_out>\n",
            "<SOLUTION>\n",
            "[Your final answer here]\n",
            "</SOLUTION>\n",
            "```\n",
            "---\n",
            "[18:08:12] ℹ️ INFO: --- Iteration Start (Rollouts: 33/1000) ---\n",
            "[18:08:12] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:08:12] ℹ️ INFO: Performing reflective mutation using indices [5 2 7]...\n",
            "[18:08:21] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:09:01] ❌ FAIL: New candidate #3 did not improve. Score: -2.25. Discarding.\n",
            "[18:09:01] ℹ️ INFO: --- Iteration Start (Rollouts: 44/1000) ---\n",
            "[18:09:01] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:09:01] ℹ️ INFO: Performing reflective mutation using indices [6 3 9]...\n",
            "[18:09:09] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:09:17] ℹ️ INFO: --- Iteration Start (Rollouts: 45/1000) ---\n",
            "[18:09:17] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:09:17] ℹ️ INFO: Performing reflective mutation using indices [2 9 3]...\n",
            "[18:09:27] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:10:07] ❌ FAIL: New candidate #3 did not improve. Score: -3.10. Discarding.\n",
            "[18:10:07] ℹ️ INFO: --- Iteration Start (Rollouts: 56/1000) ---\n",
            "[18:10:07] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:10:07] ℹ️ INFO: Performing reflective mutation using indices [3 2 7]...\n",
            "[18:10:15] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:11:02] ❌ FAIL: New candidate #3 did not improve. Score: -2.20. Discarding.\n",
            "[18:11:02] ℹ️ INFO: --- Iteration Start (Rollouts: 67/1000) ---\n",
            "[18:11:02] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:11:02] ℹ️ INFO: Performing reflective mutation using indices [7 9 5]...\n",
            "[18:11:09] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:11:18] ℹ️ INFO: --- Iteration Start (Rollouts: 68/1000) ---\n",
            "[18:11:18] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:11:18] ℹ️ INFO: Performing reflective mutation using indices [4 5 2]...\n",
            "[18:11:27] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:12:15] ❌ FAIL: New candidate #3 did not improve. Score: -2.50. Discarding.\n",
            "[18:12:15] ℹ️ INFO: --- Iteration Start (Rollouts: 79/1000) ---\n",
            "[18:12:15] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:12:15] ℹ️ INFO: Performing reflective mutation using indices [9 4 2]...\n",
            "[18:12:24] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:13:04] ❌ FAIL: New candidate #3 did not improve. Score: -1.70. Discarding.\n",
            "[18:13:04] ℹ️ INFO: --- Iteration Start (Rollouts: 90/1000) ---\n",
            "[18:13:04] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:13:04] ℹ️ INFO: Performing reflective mutation using indices [1 7 2]...\n",
            "[18:13:12] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:13:19] ℹ️ INFO: --- Iteration Start (Rollouts: 91/1000) ---\n",
            "[18:13:19] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:13:19] ℹ️ INFO: Performing reflective mutation using indices [2 6 7]...\n",
            "[18:13:26] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:13:39] ℹ️ INFO: --- Iteration Start (Rollouts: 92/1000) ---\n",
            "[18:13:39] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:13:39] ℹ️ INFO: Performing reflective mutation using indices [3 0 7]...\n",
            "[18:13:45] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:13:57] ℹ️ INFO: --- Iteration Start (Rollouts: 93/1000) ---\n",
            "[18:13:57] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:13:57] ℹ️ INFO: Performing reflective mutation using indices [8 3 1]...\n",
            "[18:14:04] ℹ️ INFO: Generated new candidate prompt #3.\n",
            "[18:14:33] ✅ SUCCESS: New candidate #3 improved! Score: -1.60 > -2.05\n",
            "[18:14:33] ℹ️ INFO: --- Iteration Start (Rollouts: 104/1000) ---\n",
            "[18:14:33] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:14:33] ℹ️ INFO: Performing reflective mutation using indices [0 8 2]...\n",
            "[18:14:46] ℹ️ INFO: Generated new candidate prompt #4.\n",
            "[18:15:39] ✅ SUCCESS: New candidate #4 improved! Score: -1.70 > -2.05\n",
            "[18:15:39] ℹ️ INFO: --- Iteration Start (Rollouts: 115/1000) ---\n",
            "[18:15:39] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:15:39] ℹ️ INFO: Performing reflective mutation using indices [7 9 5]...\n",
            "[18:16:02] ℹ️ INFO: Generated new candidate prompt #5.\n",
            "[18:17:04] ❌ FAIL: New candidate #5 did not improve. Score: -2.90. Discarding.\n",
            "[18:17:04] ℹ️ INFO: --- Iteration Start (Rollouts: 126/1000) ---\n",
            "[18:17:04] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:17:04] ℹ️ INFO: Performing reflective mutation using indices [1 3 2]...\n",
            "[18:17:12] ℹ️ INFO: Generated new candidate prompt #5.\n",
            "[18:17:40] ❌ FAIL: New candidate #5 did not improve. Score: -2.85. Discarding.\n",
            "[18:17:40] ℹ️ INFO: --- Iteration Start (Rollouts: 137/1000) ---\n",
            "[18:17:40] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:17:40] ℹ️ INFO: Performing reflective mutation using indices [7 3 4]...\n",
            "[18:17:47] ℹ️ INFO: Generated new candidate prompt #5.\n",
            "[18:18:19] ❌ FAIL: New candidate #5 did not improve. Score: -1.75. Discarding.\n",
            "[18:18:19] ℹ️ INFO: --- Iteration Start (Rollouts: 148/1000) ---\n",
            "[18:18:19] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:18:19] ℹ️ INFO: Performing reflective mutation using indices [9 2 1]...\n",
            "[18:18:26] ℹ️ INFO: Generated new candidate prompt #5.\n",
            "[18:19:01] ✅ SUCCESS: New candidate #5 improved! Score: -1.40 > -1.60\n",
            "[18:19:01] ℹ️ INFO: --- Iteration Start (Rollouts: 159/1000) ---\n",
            "[18:19:01] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:19:01] ℹ️ INFO: Performing reflective mutation using indices [2 4 8]...\n",
            "[18:19:10] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:19:20] ℹ️ INFO: --- Iteration Start (Rollouts: 160/1000) ---\n",
            "[18:19:20] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:19:20] ℹ️ INFO: Performing reflective mutation using indices [0 3 5]...\n",
            "[18:19:28] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:19:41] ℹ️ INFO: --- Iteration Start (Rollouts: 161/1000) ---\n",
            "[18:19:41] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:19:41] ℹ️ INFO: Performing reflective mutation using indices [2 3 6]...\n",
            "[18:19:48] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:20:24] ❌ FAIL: New candidate #6 did not improve. Score: -2.35. Discarding.\n",
            "[18:20:24] ℹ️ INFO: --- Iteration Start (Rollouts: 172/1000) ---\n",
            "[18:20:24] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:20:24] ℹ️ INFO: Performing reflective mutation using indices [3 4 1]...\n",
            "[18:20:31] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:20:40] ℹ️ INFO: --- Iteration Start (Rollouts: 173/1000) ---\n",
            "[18:20:40] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:20:40] ℹ️ INFO: Performing reflective mutation using indices [0 9 3]...\n",
            "[18:20:47] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:20:55] ℹ️ INFO: --- Iteration Start (Rollouts: 174/1000) ---\n",
            "[18:20:55] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:20:55] ℹ️ INFO: Performing reflective mutation using indices [8 1 6]...\n",
            "[18:21:03] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:21:09] ℹ️ INFO: --- Iteration Start (Rollouts: 175/1000) ---\n",
            "[18:21:09] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:21:09] ℹ️ INFO: Performing reflective mutation using indices [9 0 1]...\n",
            "[18:21:16] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:21:50] ❌ FAIL: New candidate #6 did not improve. Score: -2.65. Discarding.\n",
            "[18:21:50] ℹ️ INFO: --- Iteration Start (Rollouts: 186/1000) ---\n",
            "[18:21:50] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:21:50] ℹ️ INFO: Performing reflective mutation using indices [6 4 0]...\n",
            "[18:21:56] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:22:21] ❌ FAIL: New candidate #6 did not improve. Score: -2.15. Discarding.\n",
            "[18:22:21] ℹ️ INFO: --- Iteration Start (Rollouts: 197/1000) ---\n",
            "[18:22:21] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:22:21] ℹ️ INFO: Performing reflective mutation using indices [1 4 5]...\n",
            "[18:22:30] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:22:53] ℹ️ INFO: --- Iteration Start (Rollouts: 198/1000) ---\n",
            "[18:22:53] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:22:53] ℹ️ INFO: Performing reflective mutation using indices [4 7 2]...\n",
            "[18:23:02] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:23:38] ℹ️ INFO: --- Iteration Start (Rollouts: 199/1000) ---\n",
            "[18:23:38] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:23:38] ℹ️ INFO: Performing reflective mutation using indices [6 1 0]...\n",
            "[18:23:45] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:23:50] ℹ️ INFO: --- Iteration Start (Rollouts: 200/1000) ---\n",
            "[18:23:50] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:23:50] ℹ️ INFO: Performing reflective mutation using indices [6 1 3]...\n",
            "[18:23:59] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:24:23] ❌ FAIL: New candidate #6 did not improve. Score: -1.85. Discarding.\n",
            "[18:24:23] ℹ️ INFO: --- Iteration Start (Rollouts: 211/1000) ---\n",
            "[18:24:23] ℹ️ INFO: Selected candidate #5 (Score: -1.40) for mutation.\n",
            "[18:24:23] ℹ️ INFO: Performing reflective mutation using indices [9 6 2]...\n",
            "[18:24:33] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:24:43] ℹ️ INFO: --- Iteration Start (Rollouts: 212/1000) ---\n",
            "[18:24:43] ℹ️ INFO: Selected candidate #5 (Score: -1.40) for mutation.\n",
            "[18:24:43] ℹ️ INFO: Performing reflective mutation using indices [0 4 3]...\n",
            "[18:24:49] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:25:22] ❌ FAIL: New candidate #6 did not improve. Score: -2.25. Discarding.\n",
            "[18:25:22] ℹ️ INFO: --- Iteration Start (Rollouts: 223/1000) ---\n",
            "[18:25:22] ℹ️ INFO: Selected candidate #5 (Score: -1.40) for mutation.\n",
            "[18:25:22] ℹ️ INFO: Performing reflective mutation using indices [0 8 5]...\n",
            "[18:25:39] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:26:26] ❌ FAIL: New candidate #6 did not improve. Score: -2.60. Discarding.\n",
            "[18:26:26] ℹ️ INFO: --- Iteration Start (Rollouts: 234/1000) ---\n",
            "[18:26:26] ℹ️ INFO: Selected candidate #5 (Score: -1.40) for mutation.\n",
            "[18:26:27] ℹ️ INFO: Performing reflective mutation using indices [1 5 4]...\n",
            "[18:26:35] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:26:49] ℹ️ INFO: --- Iteration Start (Rollouts: 235/1000) ---\n",
            "[18:26:49] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:26:49] ℹ️ INFO: Performing reflective mutation using indices [1 9 4]...\n",
            "[18:26:57] ℹ️ INFO: Generated new candidate prompt #6.\n",
            "[18:27:26] ✅ SUCCESS: New candidate #6 improved! Score: -1.95 > -2.05\n",
            "[18:27:26] ℹ️ INFO: --- Iteration Start (Rollouts: 246/1000) ---\n",
            "[18:27:26] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:27:26] ℹ️ INFO: Performing reflective mutation using indices [9 7 8]...\n",
            "[18:27:35] ℹ️ INFO: Generated new candidate prompt #7.\n",
            "[18:28:17] ❌ FAIL: New candidate #7 did not improve. Score: -2.50. Discarding.\n",
            "[18:28:17] ℹ️ INFO: --- Iteration Start (Rollouts: 257/1000) ---\n",
            "[18:28:17] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:28:17] ℹ️ INFO: Performing reflective mutation using indices [9 3 0]...\n",
            "[18:28:24] ℹ️ INFO: Generated new candidate prompt #7.\n",
            "[18:28:50] ❌ FAIL: New candidate #7 did not improve. Score: -1.35. Discarding.\n",
            "[18:28:50] ℹ️ INFO: --- Iteration Start (Rollouts: 268/1000) ---\n",
            "[18:28:50] ℹ️ INFO: Selected candidate #5 (Score: -1.40) for mutation.\n",
            "[18:28:50] ℹ️ INFO: Performing reflective mutation using indices [5 3 1]...\n",
            "[18:28:58] ℹ️ INFO: Generated new candidate prompt #7.\n",
            "[18:29:38] ✅ SUCCESS: New candidate #7 improved! Score: -1.30 > -1.40\n",
            "[18:29:38] ℹ️ INFO: --- Iteration Start (Rollouts: 279/1000) ---\n",
            "[18:29:38] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:29:38] ℹ️ INFO: Performing reflective mutation using indices [5 1 6]...\n",
            "[18:29:58] ℹ️ INFO: Generated new candidate prompt #8.\n",
            "[18:30:27] ❌ FAIL: New candidate #8 did not improve. Score: -2.15. Discarding.\n",
            "[18:30:27] ℹ️ INFO: --- Iteration Start (Rollouts: 290/1000) ---\n",
            "[18:30:27] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:30:28] ℹ️ INFO: Performing reflective mutation using indices [8 5 2]...\n",
            "[18:30:41] ℹ️ INFO: Generated new candidate prompt #8.\n",
            "[18:31:33] ❌ FAIL: New candidate #8 did not improve. Score: -2.10. Discarding.\n",
            "[18:31:33] ℹ️ INFO: --- Iteration Start (Rollouts: 301/1000) ---\n",
            "[18:31:33] ℹ️ INFO: Selected candidate #6 (Score: -1.95) for mutation.\n",
            "[18:31:33] ℹ️ INFO: Performing reflective mutation using indices [9 0 3]...\n",
            "[18:31:40] ℹ️ INFO: Generated new candidate prompt #8.\n",
            "[18:32:18] ✅ SUCCESS: New candidate #8 improved! Score: -1.70 > -1.95\n",
            "[18:32:18] ℹ️ INFO: --- Iteration Start (Rollouts: 312/1000) ---\n",
            "[18:32:18] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:32:18] ℹ️ INFO: Performing reflective mutation using indices [5 0 3]...\n",
            "[18:32:24] ℹ️ INFO: Generated new candidate prompt #9.\n",
            "[18:32:49] ℹ️ INFO: --- Iteration Start (Rollouts: 313/1000) ---\n",
            "[18:32:49] ℹ️ INFO: Selected candidate #6 (Score: -1.95) for mutation.\n",
            "[18:32:49] ℹ️ INFO: Performing reflective mutation using indices [2 1 9]...\n",
            "[18:32:58] ℹ️ INFO: Generated new candidate prompt #9.\n",
            "[18:33:36] ❌ FAIL: New candidate #9 did not improve. Score: -2.00. Discarding.\n",
            "[18:33:36] ℹ️ INFO: --- Iteration Start (Rollouts: 324/1000) ---\n",
            "[18:33:36] ℹ️ INFO: Selected candidate #7 (Score: -1.30) for mutation.\n",
            "[18:33:36] ℹ️ INFO: Performing reflective mutation using indices [1 5 9]...\n",
            "[18:33:58] ℹ️ INFO: Generated new candidate prompt #9.\n",
            "[18:34:47] ❌ FAIL: New candidate #9 did not improve. Score: -2.25. Discarding.\n",
            "[18:34:47] ℹ️ INFO: --- Iteration Start (Rollouts: 335/1000) ---\n",
            "[18:34:47] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:34:47] ℹ️ INFO: Performing reflective mutation using indices [8 5 9]...\n",
            "[18:34:56] ℹ️ INFO: Generated new candidate prompt #9.\n",
            "[18:35:30] ❌ FAIL: New candidate #9 did not improve. Score: -2.05. Discarding.\n",
            "[18:35:30] ℹ️ INFO: --- Iteration Start (Rollouts: 346/1000) ---\n",
            "[18:35:30] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:35:30] ℹ️ INFO: Performing reflective mutation using indices [8 4 9]...\n",
            "[18:35:39] ℹ️ INFO: Generated new candidate prompt #9.\n",
            "[18:36:12] ✅ SUCCESS: New candidate #9 improved! Score: -1.45 > -1.70\n",
            "[18:36:12] ℹ️ INFO: --- Iteration Start (Rollouts: 357/1000) ---\n",
            "[18:36:12] ℹ️ INFO: Selected candidate #6 (Score: -1.95) for mutation.\n",
            "[18:36:12] ℹ️ INFO: Performing reflective mutation using indices [1 3 5]...\n",
            "[18:36:19] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:36:37] ℹ️ INFO: --- Iteration Start (Rollouts: 358/1000) ---\n",
            "[18:36:37] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:36:37] ℹ️ INFO: Performing reflective mutation using indices [2 5 1]...\n",
            "[18:36:54] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:37:26] ❌ FAIL: New candidate #10 did not improve. Score: -2.00. Discarding.\n",
            "[18:37:26] ℹ️ INFO: --- Iteration Start (Rollouts: 369/1000) ---\n",
            "[18:37:26] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:37:26] ℹ️ INFO: Performing reflective mutation using indices [7 2 1]...\n",
            "[18:37:32] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:38:30] ❌ FAIL: New candidate #10 did not improve. Score: -1.85. Discarding.\n",
            "[18:38:30] ℹ️ INFO: --- Iteration Start (Rollouts: 380/1000) ---\n",
            "[18:38:30] ℹ️ INFO: Selected candidate #7 (Score: -1.30) for mutation.\n",
            "[18:38:30] ℹ️ INFO: Performing reflective mutation using indices [1 4 7]...\n",
            "[18:38:37] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:38:43] ℹ️ INFO: --- Iteration Start (Rollouts: 381/1000) ---\n",
            "[18:38:43] ℹ️ INFO: Selected candidate #8 (Score: -1.70) for mutation.\n",
            "[18:38:43] ℹ️ INFO: Performing reflective mutation using indices [1 7 5]...\n",
            "[18:38:51] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:38:58] ℹ️ INFO: --- Iteration Start (Rollouts: 382/1000) ---\n",
            "[18:38:58] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:38:58] ℹ️ INFO: Performing reflective mutation using indices [4 0 5]...\n",
            "[18:39:05] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:39:17] ℹ️ INFO: --- Iteration Start (Rollouts: 383/1000) ---\n",
            "[18:39:17] ℹ️ INFO: Selected candidate #8 (Score: -1.70) for mutation.\n",
            "[18:39:17] ℹ️ INFO: Performing reflective mutation using indices [7 1 3]...\n",
            "[18:39:23] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:39:49] ❌ FAIL: New candidate #10 did not improve. Score: -2.10. Discarding.\n",
            "[18:39:49] ℹ️ INFO: --- Iteration Start (Rollouts: 394/1000) ---\n",
            "[18:39:49] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:39:49] ℹ️ INFO: Performing reflective mutation using indices [9 5 7]...\n",
            "[18:39:58] ℹ️ INFO: Generated new candidate prompt #10.\n",
            "[18:40:28] ✅ SUCCESS: New candidate #10 improved! Score: -1.45 > -1.70\n",
            "[18:40:28] ℹ️ INFO: --- Iteration Start (Rollouts: 405/1000) ---\n",
            "[18:40:28] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:40:28] ℹ️ INFO: Performing reflective mutation using indices [7 9 8]...\n",
            "[18:40:36] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:40:45] ℹ️ INFO: --- Iteration Start (Rollouts: 406/1000) ---\n",
            "[18:40:45] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:40:45] ℹ️ INFO: Performing reflective mutation using indices [1 3 6]...\n",
            "[18:40:53] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:41:23] ❌ FAIL: New candidate #11 did not improve. Score: -1.75. Discarding.\n",
            "[18:41:23] ℹ️ INFO: --- Iteration Start (Rollouts: 417/1000) ---\n",
            "[18:41:23] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:41:23] ℹ️ INFO: Performing reflective mutation using indices [8 0 2]...\n",
            "[18:41:30] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:42:16] ❌ FAIL: New candidate #11 did not improve. Score: -1.75. Discarding.\n",
            "[18:42:16] ℹ️ INFO: --- Iteration Start (Rollouts: 428/1000) ---\n",
            "[18:42:16] ℹ️ INFO: Selected candidate #6 (Score: -1.95) for mutation.\n",
            "[18:42:16] ℹ️ INFO: Performing reflective mutation using indices [3 7 5]...\n",
            "[18:42:23] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:42:40] ℹ️ INFO: --- Iteration Start (Rollouts: 429/1000) ---\n",
            "[18:42:40] ℹ️ INFO: Selected candidate #10 (Score: -1.45) for mutation.\n",
            "[18:42:40] ℹ️ INFO: Performing reflective mutation using indices [6 7 8]...\n",
            "[18:42:47] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:42:55] ℹ️ INFO: --- Iteration Start (Rollouts: 430/1000) ---\n",
            "[18:42:55] ℹ️ INFO: Selected candidate #9 (Score: -1.45) for mutation.\n",
            "[18:42:55] ℹ️ INFO: Performing reflective mutation using indices [2 5 4]...\n",
            "[18:43:14] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:43:59] ❌ FAIL: New candidate #11 did not improve. Score: -2.50. Discarding.\n",
            "[18:43:59] ℹ️ INFO: --- Iteration Start (Rollouts: 441/1000) ---\n",
            "[18:43:59] ℹ️ INFO: Selected candidate #10 (Score: -1.45) for mutation.\n",
            "[18:43:59] ℹ️ INFO: Performing reflective mutation using indices [4 1 3]...\n",
            "[18:44:06] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:45:01] ❌ FAIL: New candidate #11 did not improve. Score: -1.85. Discarding.\n",
            "[18:45:01] ℹ️ INFO: --- Iteration Start (Rollouts: 452/1000) ---\n",
            "[18:45:01] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:45:01] ℹ️ INFO: Performing reflective mutation using indices [8 7 3]...\n",
            "[18:45:09] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:46:06] ❌ FAIL: New candidate #11 did not improve. Score: -2.30. Discarding.\n",
            "[18:46:06] ℹ️ INFO: --- Iteration Start (Rollouts: 463/1000) ---\n",
            "[18:46:06] ℹ️ INFO: Selected candidate #6 (Score: -1.95) for mutation.\n",
            "[18:46:06] ℹ️ INFO: Performing reflective mutation using indices [0 4 8]...\n",
            "[18:46:12] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:46:18] ℹ️ INFO: --- Iteration Start (Rollouts: 464/1000) ---\n",
            "[18:46:18] ℹ️ INFO: Selected candidate #10 (Score: -1.45) for mutation.\n",
            "[18:46:18] ℹ️ INFO: Performing reflective mutation using indices [1 2 7]...\n",
            "[18:46:25] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:47:34] ❌ FAIL: New candidate #11 did not improve. Score: -1.65. Discarding.\n",
            "[18:47:34] ℹ️ INFO: --- Iteration Start (Rollouts: 475/1000) ---\n",
            "[18:47:34] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:47:34] ℹ️ INFO: Performing reflective mutation using indices [8 1 5]...\n",
            "[18:47:44] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:48:16] ❌ FAIL: New candidate #11 did not improve. Score: -1.80. Discarding.\n",
            "[18:48:16] ℹ️ INFO: --- Iteration Start (Rollouts: 486/1000) ---\n",
            "[18:48:16] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:48:16] ℹ️ INFO: Performing reflective mutation using indices [5 6 2]...\n",
            "[18:48:36] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:49:25] ❌ FAIL: New candidate #11 did not improve. Score: -2.25. Discarding.\n",
            "[18:49:25] ℹ️ INFO: --- Iteration Start (Rollouts: 497/1000) ---\n",
            "[18:49:25] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:49:25] ℹ️ INFO: Performing reflective mutation using indices [5 1 2]...\n",
            "[18:49:39] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:50:40] ❌ FAIL: New candidate #11 did not improve. Score: -1.80. Discarding.\n",
            "[18:50:40] ℹ️ INFO: --- Iteration Start (Rollouts: 508/1000) ---\n",
            "[18:50:40] ℹ️ INFO: Selected candidate #9 (Score: -1.45) for mutation.\n",
            "[18:50:40] ℹ️ INFO: Performing reflective mutation using indices [2 1 5]...\n",
            "[18:50:48] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:50:53] ℹ️ INFO: --- Iteration Start (Rollouts: 509/1000) ---\n",
            "[18:50:53] ℹ️ INFO: Selected candidate #6 (Score: -1.95) for mutation.\n",
            "[18:50:53] ℹ️ INFO: Performing reflective mutation using indices [5 8 1]...\n",
            "[18:51:06] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:51:38] ❌ FAIL: New candidate #11 did not improve. Score: -1.95. Discarding.\n",
            "[18:51:38] ℹ️ INFO: --- Iteration Start (Rollouts: 520/1000) ---\n",
            "[18:51:38] ℹ️ INFO: Selected candidate #3 (Score: -1.60) for mutation.\n",
            "[18:51:38] ℹ️ INFO: Performing reflective mutation using indices [7 2 9]...\n",
            "[18:51:46] ℹ️ INFO: Generated new candidate prompt #11.\n",
            "[18:52:25] ✅ SUCCESS: New candidate #11 improved! Score: -1.00 > -1.60\n",
            "[18:52:25] ⭐ BEST: NEW BEST PROMPT FOUND!\n",
            "Current Best Prompt:\n",
            "---\n",
            "You are a highly accurate math problem solver. Your task is to meticulously solve the given math problem step-by-step, ensuring clarity and precision in your calculations. Your final answer must be correct. Follow the instructions precisely.\n",
            "\n",
            "1.  Carefully read and thoroughly understand the problem. Identify the key information and the question being asked.\n",
            "2.  Plan your approach. Determine the necessary steps and formulas to solve the problem.\n",
            "3.  Show ALL your working out within the designated tags. Each step should be clearly explained and easy to follow.  Make sure to include an `<end_working_out>` tag after your workings.\n",
            "4.  Provide your final answer ONLY within the `<SOLUTION>` and `</SOLUTION>` tags. Ensure the answer is a single number, and nothing else.\n",
            "\n",
            "Here is the format you MUST follow:\n",
            "\n",
            "```\n",
            "<start_working_out>\n",
            "[Your step-by-step working out here]\n",
            "</start_working_out>\n",
            "<SOLUTION>\n",
            "[Your final answer here]\n",
            "</SOLUTION>\n",
            "<end_working_out>\n",
            "```\n",
            "---\n",
            "[18:52:25] ℹ️ INFO: --- Iteration Start (Rollouts: 531/1000) ---\n",
            "[18:52:25] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:52:26] ℹ️ INFO: Performing reflective mutation using indices [8 4 0]...\n",
            "[18:52:32] ℹ️ INFO: Generated new candidate prompt #12.\n",
            "[18:53:14] ❌ FAIL: New candidate #12 did not improve. Score: -2.60. Discarding.\n",
            "[18:53:14] ℹ️ INFO: --- Iteration Start (Rollouts: 542/1000) ---\n",
            "[18:53:14] ℹ️ INFO: Selected candidate #7 (Score: -1.30) for mutation.\n",
            "[18:53:14] ℹ️ INFO: Performing reflective mutation using indices [8 3 2]...\n",
            "[18:53:21] ℹ️ INFO: Generated new candidate prompt #12.\n",
            "[18:53:55] ❌ FAIL: New candidate #12 did not improve. Score: -1.70. Discarding.\n",
            "[18:53:55] ℹ️ INFO: --- Iteration Start (Rollouts: 553/1000) ---\n",
            "[18:53:55] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:53:56] ℹ️ INFO: Performing reflective mutation using indices [7 0 1]...\n",
            "[18:54:02] ℹ️ INFO: Generated new candidate prompt #12.\n",
            "[18:54:39] ❌ FAIL: New candidate #12 did not improve. Score: -3.10. Discarding.\n",
            "[18:54:39] ℹ️ INFO: --- Iteration Start (Rollouts: 564/1000) ---\n",
            "[18:54:39] ℹ️ INFO: Selected candidate #1 (Score: -2.05) for mutation.\n",
            "[18:54:39] ℹ️ INFO: Performing reflective mutation using indices [0 2 1]...\n",
            "[18:54:45] ℹ️ INFO: Generated new candidate prompt #12.\n",
            "[18:55:12] ✅ SUCCESS: New candidate #12 improved! Score: -1.55 > -2.05\n",
            "[18:55:12] ℹ️ INFO: --- Iteration Start (Rollouts: 575/1000) ---\n",
            "[18:55:12] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:55:12] ℹ️ INFO: Performing reflective mutation using indices [8 5 2]...\n",
            "[18:55:21] ℹ️ INFO: Generated new candidate prompt #13.\n",
            "[18:55:54] ❌ FAIL: New candidate #13 did not improve. Score: -1.95. Discarding.\n",
            "[18:55:54] ℹ️ INFO: --- Iteration Start (Rollouts: 586/1000) ---\n",
            "[18:55:54] ℹ️ INFO: Selected candidate #5 (Score: -1.40) for mutation.\n",
            "[18:55:54] ℹ️ INFO: Performing reflective mutation using indices [7 2 3]...\n",
            "[18:56:02] ℹ️ INFO: Generated new candidate prompt #13.\n",
            "[18:56:09] ℹ️ INFO: --- Iteration Start (Rollouts: 587/1000) ---\n",
            "[18:56:09] ℹ️ INFO: Selected candidate #4 (Score: -1.70) for mutation.\n",
            "[18:56:09] ℹ️ INFO: Performing reflective mutation using indices [8 4 5]...\n",
            "[18:56:16] ℹ️ INFO: Generated new candidate prompt #13.\n",
            "[18:56:25] ℹ️ INFO: --- Iteration Start (Rollouts: 588/1000) ---\n",
            "[18:56:25] ℹ️ INFO: Selected candidate #2 (Score: -1.10) for mutation.\n",
            "[18:56:25] ℹ️ INFO: Performing reflective mutation using indices [6 8 4]...\n",
            "[18:56:33] ℹ️ INFO: Generated new candidate prompt #13.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "#                               MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "# Set your parameters for the optimization run here.\n",
        "\n",
        "# --- Model and Prompt Settings ---\n",
        "# The \"target\" model to optimize a prompt for.\n",
        "TARGET_MODEL_NAME = \"gemma3:1b\"\n",
        "\n",
        "# The \"reflector\" model used to generate new prompt ideas.\n",
        "# It's best to use a powerful model for this, like Gemini 1.5 Pro.\n",
        "REFLECTOR_MODEL_NAME = \"gemini-2.0-flash-lite\"\n",
        "JUDGE_MODEL_NAME = \"gemini/gemini-2.0-flash-lite\"\n",
        "\n",
        "SEED_PROMPT = system_prompt\n",
        "\n",
        "# --- Training Data ---\n",
        "# Provide a list of dictionaries. Each dictionary needs an \"input\" and the\n",
        "# criteria for the evaluation_and_feedback_function (e.g., \"expected_keywords\").\n",
        "TRAINING_DATA_JSON = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"input\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
        "    \"expected_keywords\": [\"Eiffel Tower\", \"Paris\"]\n",
        "  },\n",
        "  {\n",
        "    \"input\": \"The Great Wall of China is a series of fortifications that were built across the historical northern borders of ancient Chinese states and Imperial China as protection against various nomadic groups from the Eurasian Steppe.\",\n",
        "    \"expected_keywords\": [\"Great Wall\", \"China\", \"fortifications\"]\n",
        "  },\n",
        "  {\n",
        "    \"input\": \"The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy, just east of the Roman Forum. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world today, despite its age.\",\n",
        "    \"expected_keywords\": [\"Colosseum\", \"Rome\", \"amphitheatre\"]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "TRAINING_DATA = json.loads(TRAINING_DATA_JSON)\n",
        "\n",
        "# --- Budget ---\n",
        "# Total number of times the target model will be called. Start with a small number (e.g., 10-20) to test.\n",
        "BUDGET = 1000\n",
        "\n",
        "# --- 3. Run Optimization ---\n",
        "if GEMINI_API_KEY:\n",
        "  try:\n",
        "      final_result = run_gepa_optimization(\n",
        "          target_model_name=TARGET_MODEL_NAME,\n",
        "          reflector_model_name=REFLECTOR_MODEL_NAME,\n",
        "          judge_model_name=JUDGE_MODEL_NAME,\n",
        "          seed_prompt=SEED_PROMPT,\n",
        "          training_data=train_dataset,\n",
        "          budget=BUDGET\n",
        "      )\n",
        "  except Exception as e:\n",
        "      print(f\"\\nAn unrecoverable error occurred during execution: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_prompt = \"\"\"\n",
        "You are a highly accurate problem solver and a meticulous formatter.\n",
        "\n",
        "You are given a word problem. Your task is to:\n",
        "\n",
        "1.  Thoroughly analyze the problem and create a clear, step-by-step solution.\n",
        "2.  Present your complete working out, including all calculations and logical reasoning. Your working out MUST be enclosed within the tags <start_working_out> and <end_working_out>.\n",
        "3.  Provide the final answer, enclosed within the tags <SOLUTION> and </SOLUTION>. Ensure the answer is a single numerical value and that the tags are correctly formatted.\n",
        "\n",
        "For example:\n",
        "\n",
        "Task Input: \"John has 5 apples and Mary gives him 3 more. How many apples does John have?\"\n",
        "Generated Output:\n",
        "<start_working_out>\n",
        "John starts with 5 apples.\n",
        "Mary gives him 3 more apples.\n",
        "Total apples = 5 + 3 = 8\n",
        "</end_working_out>\n",
        "<SOLUTION>8</SOLUTION>\n",
        "--------------------\n",
        "Ensure all steps are presented logically and calculations are clearly shown within the working out. Critically, ensure the closing tag </end_working_out> is always present.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_ollama_rollout_benchmark(target_model, system_prompt, question):\n",
        "#     \"\"\"\n",
        "#     Calls Ollama for the target model.\n",
        "#     This function performs a \"rollout\" for a given prompt and input.\n",
        "#     \"\"\"\n",
        "#     url = \"http://localhost:11434/api/chat\"\n",
        "#     messages = [{\"content\": system_prompt, \"role\":\"system\"}, {\"content\": question, \"role\":\"user\"}]\n",
        "#     data = {\n",
        "#         \"model\": target_model,\n",
        "#         \"messages\": messages,\n",
        "#         \"temperature\": 1,\n",
        "#         \"top_p\": 0.95,\n",
        "#         \"num_predict\": 800,  # this is Ollama's version of max_tokens,\n",
        "#         \"stream\": False\n",
        "#     }\n",
        "\n",
        "#     response = requests.post(url, json=data)\n",
        "#     response.raise_for_status()\n",
        "#     return response.json()[\"message\"][\"content\"]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_number(text):\n",
        "  match_seq = re.search(r\"(?<=<SOLUTION>)[\\d.,%$€+-]+(?=</SOLUTION>)\", text)\n",
        "  if match_seq:\n",
        "    match_text = match_seq.group()\n",
        "    match_text = match_text.replace(',', '.').replace('%', '').replace('$', '')\n",
        "    return int(float(match_text))\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing question 0 of 10\n",
            "----------------------------------\n",
            "Okay, let's break this problem down step-by-step.\n",
            "\n",
            "<start_working_out>\n",
            "Natalia sold clips to 48 of her friends in April.\n",
            "In May, she sold half as many clips as she sold in April.\n",
            "We need to find the total number of clips sold in April and May.\n",
            "</start_working_out>\n",
            "\n",
            "<end_working_out>\n",
            "First, we find the number of clips Natalia sold in April: 48 friends * 1 clip/friend = 48 clips.\n",
            "Next, we find the number of clips Natalia sold in May: 48 clips / 2 = 24 clips.\n",
            "Finally, we add the number of clips sold in April and May together: 48 + 24 = 72 clips.\n",
            "</end_working_out>\n",
            "\n",
            "<SOLUTION>72</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 72 True: 72\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Weng earns $12 an hour.\n",
            "Yesterday, she did 50 minutes of babysitting.\n",
            "We need to find out how much she earned.\n",
            "First, we convert 50 minutes to hours: 50 minutes / 60 minutes/hour = 5/6 hours\n",
            "Now, multiply the hourly rate by the number of hours worked: $12/hour * (5/6) hours = $12 * (5/6) = $12 * (5/6) = 20/6 = 10/3 = 3.333...\n",
            "Rounding to the nearest penny, Weng earned $3.33.\n",
            "</end_working_out>\n",
            "<SOLUTION>3.33</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 3 True: 10\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Let’s break down the problem step-by-step:\n",
            "1. **Calculate Betty’s initial savings:** Betty has half of $100, which is $100 / 2 = $50.\n",
            "2. **Calculate Betty’s total money:** Betty has $50 from her savings and $15 from her parents, so her total is $50 + $15 = $65.\n",
            "3. **Calculate the grandparents’ contribution:** Her grandparents gave her twice as much as her parents, who gave her $15, so the grandparents gave her $15 * 2 = $30.\n",
            "4. **Calculate the total money Betty has:** Betty now has $65 + $30 = $95.\n",
            "5. **Calculate how much more money she needs:** The wallet costs $100, and she has $95, so she needs $100 - $95 = $5 more.\n",
            "</end_working_out>\n",
            "<SOLUTION>5</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 5 True: 5\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Let’s analyze the problem step-by-step.\n",
            "\n",
            "1. **Pages read yesterday:** Julie read 12 pages yesterday.\n",
            "2. **Pages read today:** She read twice as many pages as yesterday, so she read 12 * 2 = 24 pages today.\n",
            "3. **Total pages read so far:** She has read 12 + 24 = 36 pages.\n",
            "4. **Remaining pages:** The book has 120 pages, so there are 120 - 36 = 84 pages remaining.\n",
            "5. **Pages to read tomorrow:** She wants to read half of the remaining pages, so she should read 84 / 2 = 42 pages.\n",
            "\n",
            "</start_working_out>\n",
            "<SOLUTION>42</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 42 True: 42\n",
            "----------------------------------\n",
            "Okay, let’s break this down and solve this problem.\n",
            "\n",
            "<start_working_out>\n",
            "James writes a 3-page letter to 2 different friends twice a week. We need to find out how many pages he writes in a year.\n",
            "\n",
            "First, we need to find out how many letters he writes per week. He writes 2 letters per week to each friend. So, he writes 2 * 2 = 4 letters per week.\n",
            "\n",
            "Next, we need to find out how many weeks are in a year. There are 52 weeks in a year.\n",
            "\n",
            "Now, we can find out how many letters he writes in a year by multiplying the letters per week by the number of weeks in a year.\n",
            "4 letters/week * 52 weeks/year = 208 letters per year.\n",
            "\n",
            "Therefore, James writes 208 pages in a year.\n",
            "</end_working_out>\n",
            "<SOLUTION>208</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 208 True: 624\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Let's analyze the problem step-by-step:\n",
            "1.  **Yellow flowers:** There are 10 yellow flowers.\n",
            "2.  **Purple flowers:** There are 80% more purple flowers than yellow flowers, so the number of purple flowers is 10 * 0.80 = 8 flowers.\n",
            "3.  **Green flowers:** There are 25% fewer green flowers than yellow and purple flowers combined.  The combined number of yellow and purple flowers is 10 + 8 = 18 flowers. Therefore, the number of green flowers is 18 * 0.25 = 4.5 flowers. Since we can't have half a flower, we'll round this to 5 flowers.\n",
            "4.  **Total flowers:**  The total number of flowers is the sum of yellow, purple, and green flowers: 10 + 8 + 5 = 23 flowers. </end_working_out>\n",
            "<SOLUTION>23</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 23 True: 35\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "We have 2 large pizzas, each with 16 slices.\n",
            "We have 2 small pizzas, each with 8 slices.\n",
            "Total slices from large pizzas = 2 * 16 = 32 slices.\n",
            "Total slices from small pizzas = 2 * 8 = 16 slices.\n",
            "Total slices of pizza = 32 + 16 = 48 slices.\n",
            "Albert eats all the pizza, so he eats 48 pieces.\n",
            "</end_working_out>\n",
            "<SOLUTION>48</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 48 True: 48\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Let’s break down the problem step-by-step:\n",
            "1. Initial weight: The box initially weighs 2 pounds.\n",
            "2. Jelly beans: Adding jelly beans triples the weight, so we add 2 pounds of jelly beans.  New weight = 2 + 2 = 4 pounds.\n",
            "3. More jelly beans: Adding another 2 pounds of jelly beans increases the weight by 2 pounds, making the new weight 4 + 2 = 6 pounds.\n",
            "4. Gummy worms: Adding gummy worms doubles the weight again, so we add 2 pounds of gummy worms. New weight = 6 + 2 = 8 pounds.\n",
            "Therefore, the final weight of the box of goodies is 8 pounds.</end_working_out>\n",
            "<SOLUTION>8</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 8 True: 16\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Let's break down Alexis’s expenses:\n",
            "- Shirt: $30\n",
            "- Pants: $46\n",
            "- Coat: $38\n",
            "- Socks: $11\n",
            "- Belt: $18\n",
            "- Total spent = $30 + $46 + $38 + $11 + $18 = $143\n",
            "- Budget: $200\n",
            "- Remaining budget: $200 - $143 = $57\n",
            "- Lost receipt for shoes: We need to find out how much Alexis paid for the shoes.\n",
            "- Remaining budget after shoes: $57\n",
            "</end_working_out>\n",
            "<SOLUTION>57</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 57 True: 41\n",
            "----------------------------------\n",
            "<start_working_out>\n",
            "Tina’s hourly wage is $18.00.\n",
            "She works 10 hours per day for 5 days.\n",
            "Total hours worked = 10 hours/day * 5 days = 50 hours.\n",
            "Overtime is eligible if she works more than 8 hours per shift.\n",
            "Since she works 10 hours per day, she works 8 hours per shift.\n",
            "Number of overtime hours = 10 hours - 8 hours = 2 hours.\n",
            "Overtime wage is (Hourly wage + 1/2 hourly wage) * Overtime hours.\n",
            "Overtime wage = ($18.00 + 0.5 * $18.00) * 2 = ($18.00 + 9.00) * 2 = $27.00 * 2 = $54.00.\n",
            "Total earnings = Hourly wage + Overtime wage = $18.00 + $54.00 = $72.00.\n",
            "</end_working_out>\n",
            "<SOLUTION>72.00</SOLUTION>\n",
            "============= Predictions ================\n",
            "Predicted: 72 True: 990\n"
          ]
        }
      ],
      "source": [
        "####Benchmark\n",
        "\n",
        "# Run generation + extraction\n",
        "predicted_answers = []\n",
        "reference_answers = []\n",
        "\n",
        "questions = dataset[\"question\"]\n",
        "true_answers = dataset[\"answer\"]\n",
        "\n",
        "cut = 10\n",
        "for i, (q, a) in enumerate(zip(questions[:cut], true_answers[:cut])):\n",
        "    if i % 10 == 0:\n",
        "      print(f\"Processing question {i} of {cut}\")\n",
        "    # Generate\n",
        "    gen = run_ollama_rollout(TARGET_MODEL_NAME, best_prompt, q)\n",
        "    print(\"----------------------------------\")\n",
        "    print(gen)\n",
        "    print(\"============= Predictions ================\")\n",
        "    try:\n",
        "      pred_ans = extract_number(gen)\n",
        "    except:\n",
        "      pred_ans = None\n",
        "    true_ans = a\n",
        "    print(f\"Predicted: {pred_ans} True: {true_ans}\")\n",
        "    predicted_answers.append(pred_ans)\n",
        "    reference_answers.append(true_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "for pred, true in zip(predicted_answers, reference_answers):\n",
        "  if str(pred) == str(true):\n",
        "      correct += 1\n",
        "\n",
        "print(f\"Accuracy: {correct/len(predicted_answers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answered: 1.0\n"
          ]
        }
      ],
      "source": [
        "answered = 0\n",
        "\n",
        "for pred, true in zip(predicted_answers, reference_answers):\n",
        "  if pred is not None:\n",
        "    answered += 1\n",
        "\n",
        "print(f\"Answered: {answered/len(predicted_answers)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f1a15859a540719647f8c8af2eed32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0435c54eeedc4d78848168d4361fa0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "076afdc14205410ca5d4356b377617f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0926d91ff031423f806c7a780c72a46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a2afd46122a4c20a78eebe6dbe75c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116774d7b24b4e2da5dbe87528d6c732",
            "placeholder": "​",
            "style": "IPY_MODEL_e49e4b371e584f4f972f295fe4545dfb",
            "value": "Map: 100%"
          }
        },
        "0f709f4a905e4b489af7a5b2ff0eeccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116774d7b24b4e2da5dbe87528d6c732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130985d7644a4bbebd4b4139ee948dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af4bc0256ac4b12a52afcafc535e331": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f985985947d4d01aedd13e87be37e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24dd2625515a4d9298ee9ef25dddd770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89ece5e3eae0476d82793ed3eb98d790",
            "placeholder": "​",
            "style": "IPY_MODEL_776470e9309542cfb5fd106273d063fd",
            "value": "main/train-00000-of-00001.parquet: 100%"
          }
        },
        "26a4c7f5211841d7aac1585029232046": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bcfeb5366fa45e083658dc2ffb6958e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf86bb2a2a0e45eebbc532bab1d70134",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f9dd38a164c462c9eaa536662c9ec8a",
            "value": 7473
          }
        },
        "2c533bdeb492456b96e30bcedda608bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3242ea99646e4302823a2027c37d4dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328830278211431c94433d19db84ea26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de118a428e9b40e691f14e7dd0f8b788",
              "IPY_MODEL_cb155afca1254e9891be7a28404335ac",
              "IPY_MODEL_e0cda2a187d84b1f8daf507506676cce"
            ],
            "layout": "IPY_MODEL_3784da7cc97f4c3f8f3cf81b8b69b952"
          }
        },
        "330880f9f74c42b69bd33f7762578bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3784da7cc97f4c3f8f3cf81b8b69b952": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a785b397c8e46a4800d44039db635ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4487086b4dd1483b9e8b27969037e2b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c17bf31e8e4a7db092ce103d7824d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9659dd5cb384458a172f7e16ecc8f6a",
              "IPY_MODEL_bc24f467ddff4bdabe34702f96c73bef",
              "IPY_MODEL_57d170be10f64bfa8cd05bca31ffe72f"
            ],
            "layout": "IPY_MODEL_933c9f3d5ba8407c80a9b2c25d6cd1cf"
          }
        },
        "4f54ca8a518741e5a6098ce148d423c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_076afdc14205410ca5d4356b377617f7",
            "placeholder": "​",
            "style": "IPY_MODEL_c811bf1a488c4c2084d178fc76fe8395",
            "value": " 419k/419k [00:00&lt;00:00, 780kB/s]"
          }
        },
        "4f9e725649ff45e9a9bf256a3d844200": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57d170be10f64bfa8cd05bca31ffe72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1af4bc0256ac4b12a52afcafc535e331",
            "placeholder": "​",
            "style": "IPY_MODEL_130985d7644a4bbebd4b4139ee948dcd",
            "value": " 1319/1319 [00:00&lt;00:00, 18626.73 examples/s]"
          }
        },
        "5f1d9c7ff9f1404689420ac1a949e925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_630a582e4d11424c9958ff2f326f40dd",
              "IPY_MODEL_bfde2c7bacc84f77a5a98aaf6938c59b",
              "IPY_MODEL_4f54ca8a518741e5a6098ce148d423c8"
            ],
            "layout": "IPY_MODEL_bd1ef41e403440d78e6ac16967c58f63"
          }
        },
        "5f35ff9a843b4a7ebb7a23a4ffa6ef6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f9dd38a164c462c9eaa536662c9ec8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61269fae0ce84eaf9d328adcee88e2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f985985947d4d01aedd13e87be37e5f",
            "placeholder": "​",
            "style": "IPY_MODEL_8f8e23c5d2fe4e5ba703b7d175474987",
            "value": " 2.31M/2.31M [00:00&lt;00:00, 11.2kB/s]"
          }
        },
        "630a582e4d11424c9958ff2f326f40dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6802a8b8e92a44e18bcc910bf82bf1c2",
            "placeholder": "​",
            "style": "IPY_MODEL_0f709f4a905e4b489af7a5b2ff0eeccf",
            "value": "main/test-00000-of-00001.parquet: 100%"
          }
        },
        "66fc377156c842009f48e57a8ac8ebcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6802a8b8e92a44e18bcc910bf82bf1c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fe54b3c0dda432496ed8eaf51a73ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776470e9309542cfb5fd106273d063fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89ece5e3eae0476d82793ed3eb98d790": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1e42fc0ea4459c965fbbbb270e78b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cef0abee0044328bce616e59fbcf0ba",
            "placeholder": "​",
            "style": "IPY_MODEL_01f1a15859a540719647f8c8af2eed32",
            "value": " 7473/7473 [00:00&lt;00:00, 6651.28 examples/s]"
          }
        },
        "8c33bc1304cc4e4f883543618c0d0b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f8e23c5d2fe4e5ba703b7d175474987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91b64215b8e046dfbd0e05752d4cde44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f9e725649ff45e9a9bf256a3d844200",
            "max": 2306545,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f26ec3bca385451a81dddc3d969c778e",
            "value": 2306545
          }
        },
        "933c9f3d5ba8407c80a9b2c25d6cd1cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f7c4c1dfa741b8af95137b445bee4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cef0abee0044328bce616e59fbcf0ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a082d1c66b8f40c2adf2532e8f40d4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3612d8e705e435eb5b6f8c1f405b761": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac91d346d14f45d99d956aebcb0b4edf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b9659dd5cb384458a172f7e16ecc8f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fe54b3c0dda432496ed8eaf51a73ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_330880f9f74c42b69bd33f7762578bf0",
            "value": "Generating test split: 100%"
          }
        },
        "bac5c9e89e664998b92fde78ae3c8f19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc24f467ddff4bdabe34702f96c73bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4487086b4dd1483b9e8b27969037e2b9",
            "max": 1319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97f7c4c1dfa741b8af95137b445bee4e",
            "value": 1319
          }
        },
        "bd1ef41e403440d78e6ac16967c58f63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf86bb2a2a0e45eebbc532bab1d70134": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfde2c7bacc84f77a5a98aaf6938c59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66fc377156c842009f48e57a8ac8ebcc",
            "max": 419088,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0435c54eeedc4d78848168d4361fa0b9",
            "value": 419088
          }
        },
        "c25481c025144f46b79d9c7914ad3437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24dd2625515a4d9298ee9ef25dddd770",
              "IPY_MODEL_91b64215b8e046dfbd0e05752d4cde44",
              "IPY_MODEL_61269fae0ce84eaf9d328adcee88e2d6"
            ],
            "layout": "IPY_MODEL_e76db0522da7478ab3361386fd5b325e"
          }
        },
        "c64784236a554b5fb826b805dd5f9619": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3ddb64a60ec44d18eadf475b66bcaaf",
              "IPY_MODEL_2bcfeb5366fa45e083658dc2ffb6958e",
              "IPY_MODEL_8b1e42fc0ea4459c965fbbbb270e78b1"
            ],
            "layout": "IPY_MODEL_3242ea99646e4302823a2027c37d4dd5"
          }
        },
        "c811bf1a488c4c2084d178fc76fe8395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb155afca1254e9891be7a28404335ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac91d346d14f45d99d956aebcb0b4edf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e21d2e3f073e424491be2b929b913595",
            "value": 1
          }
        },
        "d31a75a1d4de4adc9e79f682f1c5dee9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5200e65ebc94e669274f82742afa955": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c533bdeb492456b96e30bcedda608bd",
            "placeholder": "​",
            "style": "IPY_MODEL_f6d659b6d3334f369d13e2586f4138ac",
            "value": " 7473/7473 [00:00&lt;00:00, 7913.84 examples/s]"
          }
        },
        "de118a428e9b40e691f14e7dd0f8b788": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bac5c9e89e664998b92fde78ae3c8f19",
            "placeholder": "​",
            "style": "IPY_MODEL_0926d91ff031423f806c7a780c72a46b",
            "value": "README.md: "
          }
        },
        "e0cda2a187d84b1f8daf507506676cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a082d1c66b8f40c2adf2532e8f40d4a8",
            "placeholder": "​",
            "style": "IPY_MODEL_a3612d8e705e435eb5b6f8c1f405b761",
            "value": " 7.94k/? [00:00&lt;00:00, 131kB/s]"
          }
        },
        "e21d2e3f073e424491be2b929b913595": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3ddb64a60ec44d18eadf475b66bcaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a4c7f5211841d7aac1585029232046",
            "placeholder": "​",
            "style": "IPY_MODEL_5f35ff9a843b4a7ebb7a23a4ffa6ef6b",
            "value": "Generating train split: 100%"
          }
        },
        "e49e4b371e584f4f972f295fe4545dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e76db0522da7478ab3361386fd5b325e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26ec3bca385451a81dddc3d969c778e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6d659b6d3334f369d13e2586f4138ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9275585c532418ca02c36369ce11e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a2afd46122a4c20a78eebe6dbe75c96",
              "IPY_MODEL_fb1c0950d7454cd38ddc0b195cafff03",
              "IPY_MODEL_d5200e65ebc94e669274f82742afa955"
            ],
            "layout": "IPY_MODEL_d31a75a1d4de4adc9e79f682f1c5dee9"
          }
        },
        "fb1c0950d7454cd38ddc0b195cafff03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c33bc1304cc4e4f883543618c0d0b6e",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a785b397c8e46a4800d44039db635ce",
            "value": 7473
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
